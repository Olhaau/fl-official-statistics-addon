{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical insurance dataset Federated\n",
    "https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification?hl=en\n",
    "### Stand 09.11.\n",
    "* Tensorflow Federated scheint zu funktionieren\n",
    "    * Ergebnisse sehen deutlich schlechter aus als zentralisiert.\n",
    "    * MAE geht nicht unter ~8700 (vs. ~2900 im zentralisierten Modell)\n",
    "        * R² ist negativ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 17:22:00.318918: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-26 17:22:00.318967: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-26 17:22:00.350010: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-26 17:22:21.140997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-26 17:22:21.141242: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-26 17:22:21.141275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>region0</th>\n",
       "      <th>region1</th>\n",
       "      <th>region2</th>\n",
       "      <th>region3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.321227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217391</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458434</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.326087</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403820</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10600.54830</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.429379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2205.98080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.562012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1629.83350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>0.065217</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2007.94500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>0.934783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29141.36030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  sex       bmi  children  smoker  region      charges  region0  \\\n",
       "0     0.021739    0  0.321227       0.0       1       3  16884.92400        0   \n",
       "1     0.000000    1  0.479150       0.2       0       2   1725.55230        0   \n",
       "2     0.217391    1  0.458434       0.6       0       2   4449.46200        0   \n",
       "3     0.326087    1  0.181464       0.0       0       1  21984.47061        0   \n",
       "4     0.304348    1  0.347592       0.0       0       1   3866.85520        0   \n",
       "...        ...  ...       ...       ...     ...     ...          ...      ...   \n",
       "1333  0.695652    1  0.403820       0.6       0       1  10600.54830        0   \n",
       "1334  0.000000    0  0.429379       0.0       0       0   2205.98080        1   \n",
       "1335  0.000000    0  0.562012       0.0       0       2   1629.83350        0   \n",
       "1336  0.065217    0  0.264730       0.0       0       3   2007.94500        0   \n",
       "1337  0.934783    0  0.352704       0.0       1       1  29141.36030        0   \n",
       "\n",
       "      region1  region2  region3  \n",
       "0           0        0        1  \n",
       "1           0        1        0  \n",
       "2           0        1        0  \n",
       "3           1        0        0  \n",
       "4           1        0        0  \n",
       "...       ...      ...      ...  \n",
       "1333        1        0        0  \n",
       "1334        0        0        0  \n",
       "1335        0        1        0  \n",
       "1336        0        0        1  \n",
       "1337        1        0        0  \n",
       "\n",
       "[1338 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and preprocess it\n",
    "dataset = pd.read_csv('data/insurance.csv')\n",
    "# Create categorical data from non numerical features\n",
    "dataset['sex'] = dataset['sex'].astype('category').cat.codes\n",
    "dataset['region'] = dataset['region'].astype('category').cat.codes\n",
    "dataset['smoker'] = dataset['smoker'].astype('category').cat.codes\n",
    "# Min Max scale the features (which will not be one hot encoded)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(dataset[['age', 'bmi', 'children']])\n",
    "dataset[['age', 'bmi', 'children']] = scaler.transform(dataset[['age', 'bmi', 'children']])\n",
    "# Change region to an one hot encoding feature\n",
    "dataset[['region0', 'region1', 'region2', 'region3']] = pd.get_dummies(dataset['region'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and model initilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 4\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER = 20\n",
    "PREFETCH_BUFFER = 5\n",
    "NUM_ROUNDS = 150\n",
    "RUN_NAME = f'0,8-3({NUM_ROUNDS})-{NUM_EPOCHS}-epochs-{BATCH_SIZE}-batch-WithRegion/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to divide data into regions\n",
    "\n",
    "syn_samples_per_region = 1000\n",
    "\n",
    "def get_dataset_for_region(dataset, region_index, test_size_per_region=20):\n",
    "    \"\"\"Min-max scale and return data for a single, given region. The scaler must be fitted before.\n",
    "\n",
    "    :param dataset: The dataset to get the regional data from\n",
    "    :type dataset: pandas.DataFrame\n",
    "    :param region_index: The index number of the region to return\n",
    "    :type region_index: int\n",
    "    :param test_size_per_region: The amount of values to separate for testing, default are 20\n",
    "    :type test_size_per_region: int, optional\n",
    "    :return: The dataset specific for the defined region, the test values, the test labels\n",
    "    :rtype: tensorflow.python.data.ops.dataset_ops.PrefetchDataset, tuple of pandas.core.series.Series\n",
    "    \"\"\"\n",
    "    region_ds = dataset[dataset['region'] == region_index]\n",
    "    region_ds = region_ds.drop(columns=['region'])\n",
    "    len = region_ds.shape[0]\n",
    "\n",
    "    # The scaling into [0, 1] is not necessary anymore, it happens when the data loads already\n",
    "    # region_ds[['age', 'bmi', 'children']] = scaler.transform(region_ds[['age', 'bmi', 'children']])\n",
    "\n",
    "    X_test = region_ds.head(test_size_per_region)\n",
    "    y_test = X_test.pop('charges')\n",
    "\n",
    "    X_train = region_ds.tail(len - test_size_per_region)\n",
    "    y_train = X_train.pop('charges')\n",
    "\n",
    "    fed_train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
    "\n",
    "    return (\n",
    "        fed_train_dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER),\n",
    "        (X_test, y_test)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create test and train sets and put them into random_client_ds, use four clients which are independent of the region\n",
    "def get_dataset_random_region(dataset, num_clients=4, test_size_per_region=20):\n",
    "    \"\"\"Creates a list with client datasets independent of the region.\n",
    "\n",
    "    :param dataset: The dataset to get the regional data from\n",
    "    :type dataset: pandas.DataFrame\n",
    "    :param num_clients: the number of clients create (equal big datasets per client), default value is 4 clients\n",
    "    :type num_clients: int, optional\n",
    "    :param test_size_per_region: The amount of values to separate for testing, default are 20\n",
    "    :type test_size_per_region: int, optional\n",
    "    :return: List of the prepared dataset with one entry per region, the test values and labels for each region\n",
    "    :rtype: List of (tensorflow.python.data.ops.dataset_ops.PrefetchDataset, tuple of pandas.core.series.Series)\"\"\"\n",
    "    size_of_client_ds = int(dataset.shape[0] / num_clients)\n",
    "\n",
    "    dataset_to_split = dataset.copy()\n",
    "    dataset_to_split.pop(\"region\")\n",
    "    random_client_ds = []\n",
    "    for i in range(num_clients):\n",
    "        sampled = dataset_to_split.sample(n=size_of_client_ds)\n",
    "        dataset_to_split.drop(sampled.index)\n",
    "\n",
    "        X_test = sampled.head(test_size_per_region)\n",
    "        y_test = X_test.pop('charges')\n",
    "\n",
    "        X_train = sampled.tail(size_of_client_ds - test_size_per_region)\n",
    "        y_train = X_train.pop('charges')\n",
    "\n",
    "        fed_train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
    "\n",
    "        train_set = fed_train_dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)\n",
    "        test_set = (X_test, y_test)\n",
    "\n",
    "        random_client_ds.append((train_set, test_set))\n",
    "\n",
    "    return random_client_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_keras_model(input_features=9):\n",
    "    \"\"\"Create neural network for given number of input features)\n",
    "\n",
    "    :param input_features: The dimension of the first layer with represents the amount of features used\n",
    "    :type input_features: int, optional\n",
    "    :return: Created but not compiled model\n",
    "    :rtype: keras.Model\n",
    "    \"\"\"\n",
    "    return tf.keras.models.Sequential([\n",
    "        # without region: tf.keras.layers.InputLayer(input_shape=(5,)),\n",
    "        tf.keras.layers.InputLayer(input_shape=(input_features,)),\n",
    "        tf.keras.layers.Dense(16, kernel_initializer='zeros'),\n",
    "        tf.keras.layers.Dense(6, kernel_initializer='zeros'),\n",
    "        tf.keras.layers.Dense(1, kernel_initializer='zeros'),\n",
    "    ])\n",
    "\n",
    "\n",
    "# A helper function for federated learning\n",
    "def model_fn(input_features):\n",
    "    \"\"\"A function for TFF to create a local model during federated learning and return it as the correct type.\n",
    "    This model uses 9 input features i.e. the regions are part of the features.\n",
    "\n",
    "    :return: The LSTM model to be used as federated models\n",
    "    :rtype: tff.learning.Model\n",
    "    \"\"\"\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_keras_model(input_features)\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        # without region: input_spec=federated_insurance_data[0][0].element_spec,\n",
    "        input_spec=random_client_ds[0][0].element_spec,\n",
    "        loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(),\n",
    "                 # tfa.metrics.RSquare()\n",
    "                 ]\n",
    "    )\n",
    "\n",
    "# Helper functions for different features as input\n",
    "def model_fn_5():\n",
    "    return model_fn(5)\n",
    "\n",
    "def model_fn_9():\n",
    "    return model_fn(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 17:23:17.442527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-26 17:23:17.442568: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-26 17:23:17.442593: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (svsram): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# Training data for clients with regional data (each client one region)\n",
    "test_size_per_region = 20\n",
    "regions = ['region0', 'region1', 'region2', 'region3']\n",
    "federated_insurance_data = [get_dataset_for_region(dataset.drop(regions, axis=1), i, test_size_per_region=test_size_per_region)\n",
    "                            for i in range(NUM_CLIENTS-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Training data for clients with regional independent data\n",
    "random_client_ds = get_dataset_random_region(dataset, num_clients=NUM_CLIENTS, test_size_per_region=test_size_per_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with one client per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create iterative learning process which will perform the federated learning\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn_9,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.8),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 176, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 34, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 459, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 412, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 402, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 9)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create iterative learning process which will perform the federated learning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m iterative_process \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_weighted_fed_avg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fn_5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py:244\u001b[0m, in \u001b[0;36mbuild_weighted_fed_avg\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, model_distributor, model_aggregator, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    238\u001b[0m   client_work \u001b[38;5;241m=\u001b[39m model_delta_client_work\u001b[38;5;241m.\u001b[39mbuild_functional_model_delta_client_work(\n\u001b[1;32m    239\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel_fn,\n\u001b[1;32m    240\u001b[0m       optimizer\u001b[38;5;241m=\u001b[39mclient_optimizer_fn,\n\u001b[1;32m    241\u001b[0m       client_weighting\u001b[38;5;241m=\u001b[39mclient_weighting,\n\u001b[1;32m    242\u001b[0m       metrics_aggregator\u001b[38;5;241m=\u001b[39mmetrics_aggregator)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m   client_work \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_delta_client_work\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model_delta_client_work\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclient_weighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_weighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmetrics_aggregator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_aggregator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m finalizer \u001b[38;5;241m=\u001b[39m apply_optimizer_finalizer\u001b[38;5;241m.\u001b[39mbuild_apply_optimizer_finalizer(\n\u001b[1;32m    251\u001b[0m     server_optimizer_fn, model_weights_type)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m composers\u001b[38;5;241m.\u001b[39mcompose_learning_process(initial_model_weights_fn,\n\u001b[1;32m    253\u001b[0m                                           model_distributor, client_work,\n\u001b[1;32m    254\u001b[0m                                           aggregator, finalizer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:357\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work\u001b[0;34m(model_fn, optimizer, client_weighting, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    353\u001b[0m get_hparams_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    354\u001b[0m set_hparams_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@tensorflow_computation\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 357\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mclient_update_computation\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_model_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkeras_optimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m  \u001b[49m\u001b[43mclient_update\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuild_model_delta_update_with_keras_optimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_weighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:491\u001b[0m, in \u001b[0;36mComputationWrapper.__call__\u001b[0;34m(self, tff_internal_types, *args)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;66;03m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    490\u001b[0m   parameter_type \u001b[38;5;241m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 491\u001b[0m   wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfn_to_wrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# Copy the __doc__ attribute with the documentation in triple-quotes from\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# the decorated function.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m wrapped_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn_to_wrap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:218\u001b[0m, in \u001b[0;36mPythonTracingStrategy.__call__\u001b[0;34m(self, fn_to_wrap, fn_name, parameter_type, unpack)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m unpack_arguments_fn(packed_args)\n\u001b[0;32m--> 218\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mfn_to_wrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ComputationReturnedNoneError(fn_to_wrap)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:363\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work.<locals>.client_update_computation\u001b[0;34m(initial_model_weights, dataset)\u001b[0m\n\u001b[1;32m    358\u001b[0m keras_optimizer \u001b[38;5;241m=\u001b[39m optimizer()\n\u001b[1;32m    359\u001b[0m client_update \u001b[38;5;241m=\u001b[39m build_model_delta_update_with_keras_optimizer(\n\u001b[1;32m    360\u001b[0m     model_fn\u001b[38;5;241m=\u001b[39mmodel_fn,\n\u001b[1;32m    361\u001b[0m     weighting\u001b[38;5;241m=\u001b[39mclient_weighting,\n\u001b[1;32m    362\u001b[0m     use_experimental_simulation_loop\u001b[38;5;241m=\u001b[39muse_experimental_simulation_loop)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_model_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebrwxkamh.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update\u001b[0;34m(optimizer, initial_weights, data)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fscope_2\u001b[38;5;241m.\u001b[39mret(retval__2, do_return_2)\n\u001b[0;32m---> 65\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(dataset_reduce_fn), (ag__\u001b[38;5;241m.\u001b[39mld(reduce_fn), ag__\u001b[38;5;241m.\u001b[39mld(data)), \u001b[38;5;28mdict\u001b[39m(initial_state_fn\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(initial_state_for_reduce_fn)), fscope)\n\u001b[1;32m     66\u001b[0m client_update \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure, (ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msubtract, ag__\u001b[38;5;241m.\u001b[39mld(initial_weights)\u001b[38;5;241m.\u001b[39mtrainable, ag__\u001b[38;5;241m.\u001b[39mld(model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     67\u001b[0m model_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mreport_local_unfinalized_metrics, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filey92llcy4.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___dataset_reduce_fn\u001b[0;34m(reduce_fn, dataset, initial_state_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(dataset)\u001b[38;5;241m.\u001b[39mreduce, (), \u001b[38;5;28mdict\u001b[39m(initial_state\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(initial_state_fn), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), reduce_func\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(reduce_fn)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebrwxkamh.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update.<locals>.reduce_fn\u001b[0;34m(num_examples_sum, batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mforward_pass, (ag__\u001b[38;5;241m.\u001b[39mld(batch),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope_1)\n\u001b[1;32m     24\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(output)\u001b[38;5;241m.\u001b[39mloss, ag__\u001b[38;5;241m.\u001b[39mld(model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     25\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileyhh41avr.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_forward_pass, (ag__\u001b[38;5;241m.\u001b[39mld(batch_input),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6gt7sg3k.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(inputs) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body_1, else_body_1, get_state_1, set_state_1, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m predictions \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpredict_on_batch, (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(training)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (y_true,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filei_f0ju8j.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_on_batch\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_keras_model, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 176, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 34, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 459, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 412, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 402, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create iterative learning process which will perform the federated learning\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn_5,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.8),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> <\n",
      "  global_model_weights=<\n",
      "    trainable=<\n",
      "      float32[9,16],\n",
      "      float32[16],\n",
      "      float32[16,6],\n",
      "      float32[6],\n",
      "      float32[6,1],\n",
      "      float32[1]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >,\n",
      "  distributor=<>,\n",
      "  client_work=<>,\n",
      "  aggregator=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >,\n",
      "  finalizer=<\n",
      "    int64\n",
      "  >\n",
      ">@SERVER)\n"
     ]
    }
   ],
   "source": [
    "# The initial setup of the learning process\n",
    "print(iterative_process.initialize.type_signature.formatted_representation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#summary_writer = tf.summary.create_file_writer(logdir+RUN_NAME)\n",
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 41/149 [00:30<01:13,  1.47it/s]"
     ]
    }
   ],
   "source": [
    "# Train the federated model with random clients\n",
    "for round_num in tqdm.tqdm(range(1, NUM_ROUNDS)):\n",
    "    result = iterative_process.next(state, [f[0] for f in random_client_ds])\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "    for name, value in metrics['client_work']['train'].items():\n",
    "        tf.summary.scalar(name, value, step=round_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the test data for model evaluation\n",
    "X_test = pd.concat([f[1][0] for f in random_client_ds])\n",
    "y_test = pd.concat([f[1][1] for f in random_client_ds])\n",
    "\n",
    "test_sets = [tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(np.expand_dims(el[1][0], axis=0)), tf.convert_to_tensor(np.expand_dims(el[1][1], axis=0)))) for el in random_client_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "evaluation = tff.learning.build_federated_evaluation(model_fn_9)\n",
    "# print(evaluation.type_signature.formatted_representation())\n",
    "model_weights = iterative_process.get_model_weights(state)\n",
    "train_metrics = evaluation(model_weights, test_sets)\n",
    "train_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create model from training results and evaluate\n",
    "model = create_keras_model()\n",
    "model_weights.assign_weights_to(model)\n",
    "model.compile(\n",
    "    loss=tf.losses.mae,\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    metrics=[\"mae\", 'mean_squared_error', RSquare()]\n",
    ")\n",
    "# The evaluation results, for technical reasons the metrics_names is called afterwards. However, its order fits to the results\n",
    "print(model.evaluate(X_test, y_test))\n",
    "print(model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random clients (not ordered by region, as a test scenario):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "random_client_no_regions = get_dataset_random_region(dataset.drop(regions, axis=1), num_clients=NUM_CLIENTS,\n",
    "                                                     test_size_per_region=test_size_per_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PrefetchDataset element_spec=(TensorSpec(shape=(None, 9), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
       "  (           age  sex       bmi  children  smoker  region0  region1  region2  \\\n",
       "   578   0.739130    1  0.383105       0.2       0        0        0        0   \n",
       "   610   0.630435    0  0.360775       0.2       0        0        0        1   \n",
       "   569   0.652174    1  0.661959       0.4       1        0        1        0   \n",
       "   1034  0.934783    1  0.603175       0.0       0        0        1        0   \n",
       "   198   0.717391    0  0.056228       0.0       0        0        1        0   \n",
       "   981   0.347826    1  0.145682       0.0       0        1        0        0   \n",
       "   31    0.000000    0  0.278585       0.0       0        1        0        0   \n",
       "   1256  0.717391    0  0.549502       0.6       0        0        1        0   \n",
       "   1219  0.434783    0  0.383374       0.6       0        0        1        0   \n",
       "   1320  0.282609    1  0.406376       0.6       0        0        1        0   \n",
       "   613   0.347826    0  0.081786       0.6       0        1        0        0   \n",
       "   1107  0.695652    0  0.276029       0.4       0        0        1        0   \n",
       "   1263  0.543478    0  0.375034       0.2       0        0        0        0   \n",
       "   406   0.326087    0  0.224644       0.0       0        0        0        1   \n",
       "   795   0.195652    1  0.337369       0.0       1        0        1        0   \n",
       "   970   0.695652    0  0.328222       0.6       0        0        0        1   \n",
       "   824   0.913043    1  0.224913       0.0       0        0        1        0   \n",
       "   141   0.173913    1  0.444713       0.2       0        1        0        0   \n",
       "   1173  0.434783    1  0.357815       0.4       0        0        1        0   \n",
       "   1042  0.043478    1  0.396153       0.0       1        1        0        0   \n",
       "   \n",
       "         region3  \n",
       "   578         1  \n",
       "   610         0  \n",
       "   569         0  \n",
       "   1034        0  \n",
       "   198         0  \n",
       "   981         0  \n",
       "   31          0  \n",
       "   1256        0  \n",
       "   1219        0  \n",
       "   1320        0  \n",
       "   613         0  \n",
       "   1107        0  \n",
       "   1263        1  \n",
       "   406         0  \n",
       "   795         0  \n",
       "   970         0  \n",
       "   824         0  \n",
       "   141         0  \n",
       "   1173        0  \n",
       "   1042        0  ,\n",
       "   578      9724.53000\n",
       "   610      8547.69130\n",
       "   569     45702.02235\n",
       "   1034    12950.07120\n",
       "   198      9644.25250\n",
       "   981      4500.33925\n",
       "   31       2198.18985\n",
       "   1256    11436.73815\n",
       "   1219     7537.16390\n",
       "   1320     5425.02335\n",
       "   613      6753.03800\n",
       "   1107    10493.94580\n",
       "   1263     7337.74800\n",
       "   406      4185.09790\n",
       "   795     18310.74200\n",
       "   970     10702.64240\n",
       "   824     12523.60480\n",
       "   141      3490.54910\n",
       "   1173     6457.84340\n",
       "   1042    33475.81715\n",
       "   Name: charges, dtype: float64)),\n",
       " (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 9), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
       "  (           age  sex       bmi  children  smoker  region0  region1  region2  \\\n",
       "   462   0.956522    0  0.595507       0.4       0        1        0        0   \n",
       "   296   0.021739    1  0.315846       0.0       1        0        0        0   \n",
       "   821   0.173913    1  0.046005       0.0       0        0        1        0   \n",
       "   1190  0.282609    0  0.452381       0.4       0        0        1        0   \n",
       "   669   0.478261    0  0.372612       0.2       0        0        0        1   \n",
       "   639   0.826087    1  0.476190       0.8       0        0        0        1   \n",
       "   314   0.195652    0  0.415389       0.0       1        0        0        0   \n",
       "   1233  0.869565    1  0.197471       0.0       0        0        0        0   \n",
       "   933   0.586957    0  0.520312       0.0       0        0        0        0   \n",
       "   671   0.239130    0  0.408932       0.0       0        1        0        0   \n",
       "   259   0.021739    1  0.429379       0.0       1        0        1        0   \n",
       "   1321  0.956522    1  0.288808       0.0       1        1        0        0   \n",
       "   641   0.521739    1  0.332257       0.6       1        0        1        0   \n",
       "   957   0.130435    1  0.291364       0.2       0        0        1        0   \n",
       "   777   0.586957    1  0.641512       0.0       0        1        0        0   \n",
       "   1305  0.130435    0  0.316384       0.0       0        0        0        1   \n",
       "   146   0.608696    1  0.391041       0.6       1        0        1        0   \n",
       "   591   0.630435    1  0.097121       0.2       0        0        1        0   \n",
       "   297   0.630435    1  0.254237       0.2       1        0        0        1   \n",
       "   776   0.478261    1  0.439602       0.4       0        0        1        0   \n",
       "   \n",
       "         region3  \n",
       "   462         0  \n",
       "   296         1  \n",
       "   821         0  \n",
       "   1190        0  \n",
       "   669         0  \n",
       "   639         0  \n",
       "   314         1  \n",
       "   1233        1  \n",
       "   933         1  \n",
       "   671         0  \n",
       "   259         0  \n",
       "   1321        0  \n",
       "   641         0  \n",
       "   957         0  \n",
       "   777         0  \n",
       "   1305        0  \n",
       "   146         0  \n",
       "   591         0  \n",
       "   297         0  \n",
       "   776         0  ,\n",
       "   462     15230.32405\n",
       "   296     16297.84600\n",
       "   821      2680.94930\n",
       "   1190     5327.40025\n",
       "   669      6500.23590\n",
       "   639     12949.15540\n",
       "   314     34838.87300\n",
       "   1233    11345.51900\n",
       "   933      7348.14200\n",
       "   671      3943.59540\n",
       "   259     33750.29180\n",
       "   1321    28101.33305\n",
       "   641     32787.45859\n",
       "   957     12609.88702\n",
       "   777      7448.40395\n",
       "   1305     2464.61880\n",
       "   146     40720.55105\n",
       "   591      8428.06930\n",
       "   297     21978.67690\n",
       "   776      6986.69700\n",
       "   Name: charges, dtype: float64)),\n",
       " (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 9), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
       "  (           age  sex       bmi  children  smoker  region0  region1  region2  \\\n",
       "   360   0.652174    0  0.439602       0.4       0        1        0        0   \n",
       "   205   0.217391    0  0.347592       0.2       0        1        0        0   \n",
       "   468   0.217391    0  0.224913       0.2       0        1        0        0   \n",
       "   220   0.347826    0  0.477267       0.2       0        0        0        0   \n",
       "   876   0.673913    0  0.299704       0.2       0        0        0        0   \n",
       "   437   0.369565    1  0.348130       0.6       0        0        0        0   \n",
       "   1070  0.413043    1  0.567931       0.2       1        0        0        1   \n",
       "   1145  0.739130    1  0.452381       0.6       0        0        1        0   \n",
       "   55    0.869565    1  0.564837       0.4       1        0        1        0   \n",
       "   998   0.326087    0  0.546946       0.6       0        1        0        0   \n",
       "   610   0.630435    0  0.360775       0.2       0        0        0        1   \n",
       "   990   0.173913    0  0.103309       0.2       0        0        0        0   \n",
       "   1322  0.956522    1  0.615281       0.0       0        0        0        1   \n",
       "   1216  0.478261    1  0.245359       0.0       0        0        0        1   \n",
       "   325   0.478261    1  0.488162       0.2       0        1        0        0   \n",
       "   712   0.543478    0  0.396153       0.4       0        0        1        0   \n",
       "   1073  0.782609    0  0.347592       0.4       0        1        0        0   \n",
       "   180   0.869565    1  0.339925       0.0       0        0        1        0   \n",
       "   172   0.000000    1  0.000000       0.0       0        1        0        0   \n",
       "   348   0.391304    0  0.375572       0.2       0        0        0        1   \n",
       "   \n",
       "         region3  \n",
       "   360         0  \n",
       "   205         0  \n",
       "   468         0  \n",
       "   220         1  \n",
       "   876         1  \n",
       "   437         1  \n",
       "   1070        0  \n",
       "   1145        0  \n",
       "   55          0  \n",
       "   998         0  \n",
       "   610         0  \n",
       "   990         1  \n",
       "   1322        0  \n",
       "   1216        0  \n",
       "   325         0  \n",
       "   712         0  \n",
       "   1073        0  \n",
       "   180         0  \n",
       "   172         0  \n",
       "   348         0  ,\n",
       "   360     10043.24900\n",
       "   205      4337.73520\n",
       "   468     23288.92840\n",
       "   220      5012.47100\n",
       "   876     26140.36030\n",
       "   437      5926.84600\n",
       "   1070    39871.70430\n",
       "   1145    11289.10925\n",
       "   55      47496.49445\n",
       "   998      6551.75010\n",
       "   610      8547.69130\n",
       "   990      3378.91000\n",
       "   1322    12981.34570\n",
       "   1216     5415.66120\n",
       "   325      6600.20595\n",
       "   712      8310.83915\n",
       "   1073    12096.65120\n",
       "   180     11735.87905\n",
       "   172      1694.79640\n",
       "   348      5478.03680\n",
       "   Name: charges, dtype: float64)),\n",
       " (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 9), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
       "  (           age  sex       bmi  children  smoker  region0  region1  region2  \\\n",
       "   181   0.000000    0  0.600484       0.0       0        0        0        1   \n",
       "   29    0.282609    1  0.547215       0.4       1        0        0        0   \n",
       "   1323  0.521739    0  0.656712       0.4       1        0        0        1   \n",
       "   969   0.456522    0  0.493947       1.0       0        0        0        1   \n",
       "   513   0.021739    1  0.388485       0.0       0        0        0        0   \n",
       "   311   0.021739    0  0.235136       0.0       0        0        0        0   \n",
       "   459   0.478261    0  0.458434       0.6       0        0        0        1   \n",
       "   584   0.021739    1  0.127522       0.0       0        0        0        0   \n",
       "   308   0.869565    1  0.508609       0.0       0        1        0        0   \n",
       "   598   0.543478    1  0.447673       0.4       0        0        0        0   \n",
       "   42    0.500000    1  0.156578       0.2       0        0        0        1   \n",
       "   477   0.152174    1  0.529056       0.0       0        0        1        0   \n",
       "   587   0.347826    0  0.383374       0.2       1        0        1        0   \n",
       "   1027  0.108696    1  0.074119       0.0       0        0        1        0   \n",
       "   255   0.804348    0  0.253027       0.6       0        1        0        0   \n",
       "   665   0.543478    1  0.594566       0.4       1        0        0        1   \n",
       "   625   0.239130    0  0.270917       0.0       0        0        1        0   \n",
       "   1034  0.934783    1  0.603175       0.0       0        0        1        0   \n",
       "   597   0.347826    0  0.465160       0.2       0        1        0        0   \n",
       "   116   0.869565    1  0.890503       0.0       0        0        0        1   \n",
       "   \n",
       "         region3  \n",
       "   181         0  \n",
       "   29          1  \n",
       "   1323        0  \n",
       "   969         0  \n",
       "   513         1  \n",
       "   311         1  \n",
       "   459         0  \n",
       "   584         1  \n",
       "   308         0  \n",
       "   598         1  \n",
       "   42          0  \n",
       "   477         0  \n",
       "   587         0  \n",
       "   1027        0  \n",
       "   255         0  \n",
       "   665         0  \n",
       "   625         0  \n",
       "   1034        0  \n",
       "   597         0  \n",
       "   116         0  ,\n",
       "   181      1631.82120\n",
       "   29      38711.00000\n",
       "   1323    43896.37630\n",
       "   969      8596.82780\n",
       "   513      1256.29900\n",
       "   311      1737.37600\n",
       "   459      7682.67000\n",
       "   584      1242.81600\n",
       "   308     11944.59435\n",
       "   598      7441.50100\n",
       "   42       6272.47720\n",
       "   477      2534.39375\n",
       "   587     43943.87610\n",
       "   1027    21595.38229\n",
       "   255     13047.33235\n",
       "   665     42560.43040\n",
       "   625      3736.46470\n",
       "   1034    12950.07120\n",
       "   597      5594.84550\n",
       "   116     11381.32540\n",
       "   Name: charges, dtype: float64))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_client_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_size_per_client = 20\n",
    "size_of_client_ds = int(dataset.shape[0] / 4)\n",
    "\n",
    "dataset_to_split = dataset.copy()\n",
    "random_client_ds = []\n",
    "for i in range(4):\n",
    "    sampled = dataset_to_split.sample(n=size_of_client_ds)\n",
    "    dataset_to_split.drop(sampled.index)\n",
    "\n",
    "    X_test = sampled.head(test_size_per_region)\n",
    "    y_test = X_test.pop('charges')\n",
    "\n",
    "    X_train = sampled.tail(size_of_client_ds - test_size_per_region)\n",
    "    #X_train = pd.concat([X_train, syn_region_ds], sort=False)\n",
    "    y_train = X_train.pop('charges')\n",
    "\n",
    "    fed_train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
    "\n",
    "    train_set = fed_train_dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).prefetch(\n",
    "        PREFETCH_BUFFER)\n",
    "    test_set = (X_test, y_test)\n",
    "\n",
    "    random_client_ds.append((train_set, test_set))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 176, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 34, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 459, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 412, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 402, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 9)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m iterative_process \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_weighted_fed_avg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fn_5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py:244\u001b[0m, in \u001b[0;36mbuild_weighted_fed_avg\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, model_distributor, model_aggregator, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    238\u001b[0m   client_work \u001b[38;5;241m=\u001b[39m model_delta_client_work\u001b[38;5;241m.\u001b[39mbuild_functional_model_delta_client_work(\n\u001b[1;32m    239\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel_fn,\n\u001b[1;32m    240\u001b[0m       optimizer\u001b[38;5;241m=\u001b[39mclient_optimizer_fn,\n\u001b[1;32m    241\u001b[0m       client_weighting\u001b[38;5;241m=\u001b[39mclient_weighting,\n\u001b[1;32m    242\u001b[0m       metrics_aggregator\u001b[38;5;241m=\u001b[39mmetrics_aggregator)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m   client_work \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_delta_client_work\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model_delta_client_work\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclient_weighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_weighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmetrics_aggregator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_aggregator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m finalizer \u001b[38;5;241m=\u001b[39m apply_optimizer_finalizer\u001b[38;5;241m.\u001b[39mbuild_apply_optimizer_finalizer(\n\u001b[1;32m    251\u001b[0m     server_optimizer_fn, model_weights_type)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m composers\u001b[38;5;241m.\u001b[39mcompose_learning_process(initial_model_weights_fn,\n\u001b[1;32m    253\u001b[0m                                           model_distributor, client_work,\n\u001b[1;32m    254\u001b[0m                                           aggregator, finalizer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:357\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work\u001b[0;34m(model_fn, optimizer, client_weighting, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    353\u001b[0m get_hparams_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    354\u001b[0m set_hparams_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@tensorflow_computation\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 357\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mclient_update_computation\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_model_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkeras_optimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m  \u001b[49m\u001b[43mclient_update\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuild_model_delta_update_with_keras_optimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_weighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_experimental_simulation_loop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:491\u001b[0m, in \u001b[0;36mComputationWrapper.__call__\u001b[0;34m(self, tff_internal_types, *args)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;66;03m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    490\u001b[0m   parameter_type \u001b[38;5;241m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 491\u001b[0m   wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfn_to_wrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# Copy the __doc__ attribute with the documentation in triple-quotes from\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# the decorated function.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m wrapped_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn_to_wrap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:218\u001b[0m, in \u001b[0;36mPythonTracingStrategy.__call__\u001b[0;34m(self, fn_to_wrap, fn_name, parameter_type, unpack)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m unpack_arguments_fn(packed_args)\n\u001b[0;32m--> 218\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mfn_to_wrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ComputationReturnedNoneError(fn_to_wrap)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:363\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work.<locals>.client_update_computation\u001b[0;34m(initial_model_weights, dataset)\u001b[0m\n\u001b[1;32m    358\u001b[0m keras_optimizer \u001b[38;5;241m=\u001b[39m optimizer()\n\u001b[1;32m    359\u001b[0m client_update \u001b[38;5;241m=\u001b[39m build_model_delta_update_with_keras_optimizer(\n\u001b[1;32m    360\u001b[0m     model_fn\u001b[38;5;241m=\u001b[39mmodel_fn,\n\u001b[1;32m    361\u001b[0m     weighting\u001b[38;5;241m=\u001b[39mclient_weighting,\n\u001b[1;32m    362\u001b[0m     use_experimental_simulation_loop\u001b[38;5;241m=\u001b[39muse_experimental_simulation_loop)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_model_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileskvl5mdo.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update\u001b[0;34m(optimizer, initial_weights, data)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fscope_2\u001b[38;5;241m.\u001b[39mret(retval__2, do_return_2)\n\u001b[0;32m---> 65\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(dataset_reduce_fn), (ag__\u001b[38;5;241m.\u001b[39mld(reduce_fn), ag__\u001b[38;5;241m.\u001b[39mld(data)), \u001b[38;5;28mdict\u001b[39m(initial_state_fn\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(initial_state_for_reduce_fn)), fscope)\n\u001b[1;32m     66\u001b[0m client_update \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure, (ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msubtract, ag__\u001b[38;5;241m.\u001b[39mld(initial_weights)\u001b[38;5;241m.\u001b[39mtrainable, ag__\u001b[38;5;241m.\u001b[39mld(model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     67\u001b[0m model_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mreport_local_unfinalized_metrics, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileh66ef9ms.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___dataset_reduce_fn\u001b[0;34m(reduce_fn, dataset, initial_state_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(dataset)\u001b[38;5;241m.\u001b[39mreduce, (), \u001b[38;5;28mdict\u001b[39m(initial_state\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(initial_state_fn), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), reduce_func\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(reduce_fn)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileskvl5mdo.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update.<locals>.reduce_fn\u001b[0;34m(num_examples_sum, batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mforward_pass, (ag__\u001b[38;5;241m.\u001b[39mld(batch),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope_1)\n\u001b[1;32m     24\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(output)\u001b[38;5;241m.\u001b[39mloss, ag__\u001b[38;5;241m.\u001b[39mld(model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     25\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileiuj_ec6f.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_forward_pass, (ag__\u001b[38;5;241m.\u001b[39mld(batch_input),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezknwbktg.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(inputs) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body_1, else_body_1, get_state_1, set_state_1, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m predictions \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpredict_on_batch, (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(training)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (y_true,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewp_s403a.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_on_batch\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_keras_model, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 176, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 34, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 459, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 412, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 402, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/informatik1/students/home/0zadim/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 9)\n"
     ]
    }
   ],
   "source": [
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn_5,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.8),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "RUN_NAME = '0,8-3(150)-5-epochs-10-batch-WithRegion/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> <\n",
      "  global_model_weights=<\n",
      "    trainable=<\n",
      "      float32[9,16],\n",
      "      float32[16],\n",
      "      float32[16,6],\n",
      "      float32[6],\n",
      "      float32[6,1],\n",
      "      float32[1]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >,\n",
      "  distributor=<>,\n",
      "  client_work=<>,\n",
      "  aggregator=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >,\n",
      "  finalizer=<\n",
      "    int64\n",
      "  >\n",
      ">@SERVER)\n"
     ]
    }
   ],
   "source": [
    "print(iterative_process.initialize.type_signature.formatted_representation())\n",
    "NUM_ROUNDS = 150\n",
    "#@test {\"skip\": true}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf INFO google/protobuf/util/message_differencer.cc:1419] Proto type 'tensorflow.GraphDef' not found\n",
      "[libprotobuf INFO google/protobuf/util/message_differencer.cc:1419] Proto type 'tensorflow.GraphDef' not found\n"
     ]
    }
   ],
   "source": [
    "#logdir = \"/tmp/logs/scalars/training/\"\n",
    "#summary_writer = tf.summary.create_file_writer(logdir + RUN_NAME)\n",
    "state = iterative_process.initialize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "#with summary_writer.as_default():\n",
    "for round_num in range(1, NUM_ROUNDS):\n",
    "    result = iterative_process.next(state, [f[0] for f in random_client_ds])  #  [f[0] for f in federated_insurance_data])\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "    for name, value in metrics['client_work']['train'].items():\n",
    "        tf.summary.scalar(name, value, step=round_num)\n",
    "#@test {\"skip\": true}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.concat([f[1][0] for f in random_client_ds])\n",
    "y_test = pd.concat([f[1][1] for f in random_client_ds])\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_test), tf.convert_to_tensor(y_test)))\n",
    "\n",
    "test_sets = [tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.convert_to_tensor(np.expand_dims(el[1][0], axis=0)), tf.convert_to_tensor(np.expand_dims(el[1][1], axis=0))))\n",
    "    for el in random_client_ds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf INFO google/protobuf/util/message_differencer.cc:1419] Proto type 'tensorflow.GraphDef' not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"OrderedDict([('eval', OrderedDict([('mean_absolute_error', 8133.467), ('loss', 8133.4673), ('num_examples', 80), ('num_batches', 4)]))])\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
    "#print(evaluation.type_signature.formatted_representation())\n",
    "model_weights = iterative_process.get_model_weights(state)\n",
    "train_metrics = evaluation(model_weights, test_sets)\n",
    "str(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 8413.9219 - mae: 8413.9219 - mean_squared_error: 170050112.0000 - r_square: -0.1058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8413.921875, 8413.921875, 170050112.0, -0.1058434247970581]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = create_keras_model()\n",
    "model_weights.assign_weights_to(model)\n",
    "model.compile(\n",
    "    loss=tf.losses.mae,\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    metrics=[\"mae\", 'mean_squared_error', RSquare()]\n",
    ")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
