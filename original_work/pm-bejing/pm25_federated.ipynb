{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A Federated model for PM2.5 prediction#\n",
    "This notebook creates federated models for the PM2.5 prediction in Beijing based on pm25_beijing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from datetime import datetime\n",
    "\n",
    "from pm25_beijing import DataHandler, fed_model_fn, create_fed_lstm\n",
    "\n",
    "DATA_PATH = \"data/pollution-data/\"\n",
    "FEATURES_TO_USE = [\"TEMP\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\", \"wd\", \"month\", \"day\", \"hour\"]\n",
    "TIMESTEPS = 48 # How many steps the LSTM should take into account\n",
    "NUM_REG_CLASSES = 3\n",
    "TRAIN_SPLIT = 0.25\n",
    "BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 192\n",
    "lr_client = 0.1\n",
    "lr_server = 1\n",
    "FEDERATED_TRAINING_ROUNDS = 300\n",
    "REDUCE_LR_EVERY = 64\n",
    "MODEL_PATH_PREFIX = \"models/federated/\"\n",
    "LOGFILE_PREFIX = \"tensorboard-logs/\"\n",
    "infos = f\"reduce-ev{REDUCE_LR_EVERY}-b-{BATCH_SIZE}-lstm-SGD{lr_client}-SGD{lr_server}\"\n",
    "logfile = f'{LOGFILE_PREFIX}{datetime.now()} {infos}'  # für tensorboard log-Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized wd (wind direction) as feature. Create columns north, east, south and west automatically.\n",
      "Creating multiple classes from wd (wind direction):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:46<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'wd' is not part of the features anymore. Instead each wind direction is separated (north, east, south, west)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data\n",
    "data = DataHandler(DATA_PATH, features_to_use=FEATURES_TO_USE)\n",
    "data.preprocess_data(minmax_features=FEATURES_TO_USE)\n",
    "data.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 1 of 12...\n",
      "Aotizhongxin (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:38<00:00, 915.79it/s] \n",
      "2023-01-19 20:08:20.018190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-19 20:08:20.018249: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-19 20:08:20.018284: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (svsram): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 2 of 12...\n",
      "Changping (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:37<00:00, 931.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 3 of 12...\n",
      "Dingling (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:38<00:00, 912.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 4 of 12...\n",
      "Dongsi (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:38<00:00, 917.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 5 of 12...\n",
      "Guanyuan (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:37<00:00, 926.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 6 of 12...\n",
      "Gucheng (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:38<00:00, 913.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 7 of 12...\n",
      "Huairou (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:37<00:00, 925.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 8 of 12...\n",
      "Nongzhanguan (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:38<00:00, 916.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 9 of 12...\n",
      "Shunyi (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:37<00:00, 930.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 10 of 12...\n",
      "Tiantan (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:37<00:00, 921.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 11 of 12...\n",
      "Wanliu (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:38<00:00, 913.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data 12 of 12...\n",
      "Wanshouxigong (1/1)\n",
      "Creating model input from ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'month', 'day', 'hour', 'wd_N', 'wd_E', 'wd_S', 'wd_W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35016/35016 [00:37<00:00, 926.10it/s] \n"
     ]
    }
   ],
   "source": [
    "federated_train_data = []\n",
    "federated_test_data = None\n",
    "federated_test_labels = []\n",
    "i = 0\n",
    "for station in data.station:\n",
    "    i = i + 1\n",
    "    pm_data, labels = data.create_classes(NUM_REG_CLASSES, station=station)\n",
    "    nr_train = int(TRAIN_SPLIT*len(data.data[station]))\n",
    "    nr_test = len(data.data[station]) - nr_train\n",
    "    print(f\"Assembling data {i} of {len(data.station)}...\")\n",
    "    train_data_orig = data.create_model_input(TIMESTEPS, station=[station])\n",
    "\n",
    "    test_data = train_data_orig[:nr_train]\n",
    "    train_data = train_data_orig[nr_train:]\n",
    "    train_labels = labels[nr_train:-TIMESTEPS]\n",
    "    test_labels = labels[:nr_train]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(train_data), tf.convert_to_tensor(train_labels)))\n",
    "    federated_train_data.append(ds)\n",
    "    if federated_test_data is None:\n",
    "        federated_test_data = test_data\n",
    "    else:\n",
    "        federated_test_data = np.concatenate((federated_test_data, test_data))\n",
    "    federated_test_labels.append(test_labels)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(federated_test_data),\n",
    "                                              tf.convert_to_tensor(pd.concat(federated_test_labels))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batched_federated_train_data = [ds.batch(BATCH_SIZE) for ds in federated_train_data]\n",
    "batched_test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Tensorboard use\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir tensorboard-logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf INFO google/protobuf/util/message_differencer.cc:1419] Proto type 'tensorflow.GraphDef' not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> <\n",
      "  global_model_weights=<\n",
      "    trainable=<\n",
      "      float32[12,24],\n",
      "      float32[6,24],\n",
      "      float32[24],\n",
      "      float32[6,16],\n",
      "      float32[4,16],\n",
      "      float32[16],\n",
      "      float32[4,3],\n",
      "      float32[3]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >,\n",
      "  distributor=<>,\n",
      "  client_work=int32,\n",
      "  aggregator=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >,\n",
      "  finalizer=<\n",
      "    int64\n",
      "  >\n",
      ">@SERVER)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/299 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summary_writer\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m round_num \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, FEDERATED_TRAINING_ROUNDS)):\n\u001b[0;32m---> 16\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43miterative_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched_federated_train_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         state \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m     18\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_impl.py:135\u001b[0m, in \u001b[0;36mConcreteComputation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    134\u001b[0m   arg \u001b[38;5;241m=\u001b[39m function_utils\u001b[38;5;241m.\u001b[39mpack_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type_signature\u001b[38;5;241m.\u001b[39mparameter, args, kwargs)\n\u001b[0;32m--> 135\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/execution_contexts/sync_execution_context.py:67\u001b[0m, in \u001b[0;36mExecutionContext.invoke\u001b[0;34m(self, comp, arg)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, comp, arg):\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_coro_and_return_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_federated/python/common_libs/async_utils.py:223\u001b[0m, in \u001b[0;36mAsyncThreadRunner.run_coro_and_return_result\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"Runs coroutine in the managed event loop, returning the result.\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_loop)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The federated learning\n",
    "tff.federated_computation(lambda: 'Initialized!')()\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg_with_optimizer_schedule(\n",
    "    fed_model_fn,\n",
    "    client_learning_rate_fn=lambda round_n: lr_client/10**tf.math.floor(tf.divide(round_n, REDUCE_LR_EVERY)),\n",
    "    client_optimizer_fn=lambda x: tf.keras.optimizers.SGD(learning_rate=x),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=lr_server)\n",
    ")\n",
    "\n",
    "print(iterative_process.initialize.type_signature.formatted_representation())\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(logfile)\n",
    "with summary_writer.as_default():\n",
    "    for round_num in tqdm.tqdm(range(1, FEDERATED_TRAINING_ROUNDS)):\n",
    "        result = iterative_process.next(state, batched_federated_train_data)\n",
    "        state = result.state\n",
    "        metrics = result.metrics\n",
    "        for name, value in metrics['client_work']['train'].items():\n",
    "            tf.summary.scalar(name, value, step=round_num)\n",
    "        # Test resulting model\n",
    "        model = create_fed_lstm()\n",
    "        model_weights = iterative_process.get_model_weights(state)\n",
    "        model_weights.assign_weights_to(model)\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            optimizer='sgd',\n",
    "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "        )\n",
    "        scores = model.evaluate(batched_test_ds, verbose=0)\n",
    "        tf.summary.scalar('test_accuracy', scores[1], step=round_num)\n",
    "        tf.summary.scalar('test_precision', scores[2], step=round_num)\n",
    "        tf.summary.scalar('test_recall', scores[3], step=round_num)\n",
    "        lr = lr_client/(10**(int(round_num/REDUCE_LR_EVERY)))\n",
    "        tf.summary.scalar('z_learning_rate', lr, step=round_num)\n",
    "\n",
    "print(f'\\nFINISHED federated training, logfile: {logfile}, NOW: {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_208_layer_call_fn, lstm_cell_208_layer_call_and_return_conditional_losses, lstm_cell_209_layer_call_fn, lstm_cell_209_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/federated/federated_model_2023-01-19 21:06:18.691249/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/federated/federated_model_2023-01-19 21:06:18.691249/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"{MODEL_PATH_PREFIX}federated_model_{datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
