{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c22209-c976-434a-b3e7-e6d7f7d5389e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T11:57:47.295804Z",
     "iopub.status.busy": "2023-03-28T11:57:47.295804Z",
     "iopub.status.idle": "2023-03-28T11:57:47.299339Z",
     "shell.execute_reply": "2023-03-28T11:57:47.299339Z",
     "shell.execute_reply.started": "2023-03-28T11:57:47.295804Z"
    },
    "tags": []
   },
   "source": [
    "# Federated Learning in Official Statistics - Initial Results\n",
    "\n",
    "This notebook presents the **initial results of the originating work** <a name=\"cite_ref-1\"></a>[(Stock et al., 2023)](#cite_note-1).\n",
    "\n",
    "The associated code can be found in the folder 'original_work' of this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc4629-1a7d-44bb-97b6-3b218f91d0d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Object of Investigation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7a608-cd8d-47c2-b4d7-7718354abb88",
   "metadata": {},
   "source": [
    "To analyze the potential of FL for official statistics, <a name=\"cite_ref-1\"></a>[(Stock et al., 2023)](#cite_note-1) ran three simulations with different datasets:\n",
    "\n",
    "1. **medical insurance** (presumably artificial): available at https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset\n",
    "2. **LTE** (mobile radio): privately held by the company https://www.umlaut.com, not publically available\n",
    "3. **Pollution** (of fine dust PM<sub>2.5</sub> in Bejing): available at https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Datahttps://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02dab58-1acb-4138-aca0-647d9fff46ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Medical Insurance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ad73a-e6f3-4a1d-ab00-ec84fbabaca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T13:11:16.906742Z",
     "iopub.status.busy": "2023-03-28T13:11:16.906742Z",
     "iopub.status.idle": "2023-03-28T13:11:16.926752Z",
     "shell.execute_reply": "2023-03-28T13:11:16.925751Z",
     "shell.execute_reply.started": "2023-03-28T13:11:16.906742Z"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Remark:</b>  A minimal and lightweight use case that addresses a real privacy problem related to official statistics, and identifies opportunities for improving performance in the decentralized setting. This use case is being used to initialize a Federated Learning infrastructure.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b524f7-fc2c-4f0f-9049-b7702eaaa70c",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d284f-7efe-4792-9572-d30b71e23ed9",
   "metadata": {},
   "source": [
    "Data about medical insurance costs of individual persons.\n",
    "\n",
    "- 1338 records\n",
    "- 8 attributes: age, gender, bmi, children, smoker, region, charges\n",
    "- presumably artificial: complete (no missings), balanced in age, gender \n",
    "\n",
    "As the ML task, the following regression problem is investigated: \n",
    "> *Given the other attributes of the data set, how high are the insurance charges of an individual?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a83b79-4e51-4410-be56-a98a091e7c85",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Minimal: Scaling + Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b76fe-cceb-4e89-af62-1a1ba4d9003f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a45958-7259-4290-aa89-fbfcf86bb695",
   "metadata": {},
   "source": [
    "> *The two centralized learning approaches random forest and neural network are used as benchmarks for the FL scenario. A hyperparameter search yields the hyperparameters in Table 2. For the second benchmark (neural network), a hyperparameter search yields the following architecture: 16 dense-neurons in the first layer and 2 dense-neurons in the second layer, followed by a single dense-neuron in the output layer. The neural network is trained for 100 epochs, using the stochastic gradient descent (SGD) implementation by TensorFlow.* \n",
    "\n",
    "> *For the FL scenario, we use a slightly larger neural network with 16 dense-neurons in the first layer and 6 neurons in the second layer, again followed by a single neuron in the output layer. We run the FL training process for 150 rounds, with SGD and a learning rate of 0.8 for the clients and 3.0 for the server.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c5317-b3ee-4def-9dfc-0b6aa4380ce7",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525624c-8dc8-4bf9-a902-8cc32134fb07",
   "metadata": {},
   "source": [
    "Performance is measured by a $R^2$ score of a test set (holdout evaluation).\n",
    "\n",
    "- Benchmarks (centralized)\n",
    "    - 0.877 random forest regressor\n",
    "    - 0.85 neural network\n",
    "- Federated Learning\n",
    "    - -0.075\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6163adb9-b46b-4145-9490-32592296841f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T15:40:03.744334Z",
     "iopub.status.busy": "2023-03-28T15:40:03.744334Z",
     "iopub.status.idle": "2023-03-28T15:40:03.757213Z",
     "shell.execute_reply": "2023-03-28T15:40:03.757213Z",
     "shell.execute_reply.started": "2023-03-28T15:40:03.744334Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\olisc\\\\OneDrive\\\\code\\\\py\\\\fl2\\\\fl-official-statistics-addon\\\\_dev'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba34784-39b5-42a5-a89e-8cab319beab5",
   "metadata": {},
   "source": [
    "### Our Investigation\n",
    "\n",
    "We investigated in [med-insurance-federated.ipynb](../original_work/med-insurance/med-insurance-federated.ipynb), why FL performs so much worse. Our Observations:\n",
    "\n",
    "- the FL model was not the DNN used in the centralized situation\n",
    "- the FL model didn't learn anything (zero initializer)\n",
    "- in the FL use case, we tested other model specifications and couldn't improve the performance.\n",
    "\n",
    "\n",
    "Thus, we switch to investigating the centralized DNN.\n",
    "\n",
    "tba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e7f05-5a87-4a38-8d94-31d4be39d03d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LTE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111fed2-b859-4c4b-9bf0-34d17df3f53b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Remark</b> The use case is currently not being investigated further because the data is not available. Any further investigation would require umlaut's participation to access the data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d65b9e-e902-45be-8d72-158974afa8a6",
   "metadata": {},
   "source": [
    "more tba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b998b9-9caa-485f-98f7-e1f530a2809a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e395f2-b322-43e2-8504-f47cf90a2467",
   "metadata": {},
   "source": [
    "Results The benchmarks of the centralized learning regressors are \n",
    "\n",
    "- $R^2 = 0.158$ (random forest)\n",
    "- $R^2 = 0.13$ (neural network)\n",
    "- $R^2 = 0.13$ (linear regression) \n",
    "- $R^2 = 0.114$ (neural network - Federated Learning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb6481-4d64-4874-9049-44f507ebfe7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pollution\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5534382-7c4e-4bae-8718-36d065a11247",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Remark</b> Very good performance, but no real privacy issues. A good use case for upcomming technical tests, but no suitable product for official statistics and to present the advantages of Federated Learning.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b590de7-7381-4d02-abe8-8dc2eb4ac233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:32:55.478388Z",
     "iopub.status.busy": "2023-03-28T14:32:55.478388Z",
     "iopub.status.idle": "2023-03-28T14:32:55.491135Z",
     "shell.execute_reply": "2023-03-28T14:32:55.491135Z",
     "shell.execute_reply.started": "2023-03-28T14:32:55.478388Z"
    },
    "tags": []
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275478c-b2c1-4ba7-88b7-3ec0c482cec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "> *We model a classification task in which the current fine dust pollution is inferred based on meteorological input data. More precisely, 48 consecutive hourly measurements are used to make a prediction for the current PM2.5 pollution (the total weight of particles smaller than 2.5 Î¼m in one m3). The output of the predictor is one of the three classes low, medium or high. The threshold for each class are chosen in a way such that the samples of the whole data set are distributed evenly among the three classes.* \n",
    "\n",
    "#### Dataset\n",
    "\n",
    "> *The data set we use is a multi-feature air quality and weather data set. It consists of hourly measurements of 12 meteorological stations in Beijing, recorded over a time span of 4 years (2013â2017). In total, more than 420 000 data records are included in the data set. Although some attributes are missing for some data records, most records have data for all of a total of 17 attributes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb348c0-96b5-4611-8a74-9aafa9c13dd6",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51acc2-e4ce-4065-8cab-d65d63e5f36a",
   "metadata": {},
   "source": [
    "> *To complete the missing data records, we use linear interpolation. We encode the wind direction by parsing the wd attribute into four binary attributes (one for each cardinal direction). All other features are scaled using a standard scaler implementation. We exclude the following pollution features from training, since we expect a high correlation with the target attribute PM2.5: PM10, SO2, NO2, CO and O3.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf9d26-28b7-4985-886a-681ebd719cd2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084ba61-fb8d-4932-b16c-543ca720e78e",
   "metadata": {},
   "source": [
    "> *We use the same model architecture for all three scenarios, a neural network with five layers: A 10-neuron LSTM (long-short term memory) layer, a dropout layer with a dropout rate of 25%, a 5-neuron LSTM layer, another dropout layer with a dropout rate of 35% and a 3-neuron dense layer for the classification output.*\n",
    "\n",
    "> *We train for 20 epochs in the first scenario, 10 epochs in the second scenario and 160 epochs in third scenario (FL). In all scenarios, we use CategoricalCrossEntropy as the loss function. While we use the Adam optimizer with an automatic learning rate in both of the centralized learning scenarios, we employ the Stochastic Gradient Descent (SGD) optimizer in the FL scenario. On the server we use a learning rate of 1 for SGD, on the client we start with a learning rate of 0.1. The latter is divided by 10 every 64 rounds, such that at the end of the 160 epochs in the FL scenario, the client learning rate is at 0.001.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05606ddf-fb0e-4c06-b5c1-1203ed6e2a9b",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3d293-3594-4e36-a4d3-133bd92f6e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:24:49.539812Z",
     "iopub.status.busy": "2023-03-28T14:24:49.538811Z",
     "iopub.status.idle": "2023-03-28T14:24:49.563681Z",
     "shell.execute_reply": "2023-03-28T14:24:49.562635Z",
     "shell.execute_reply.started": "2023-03-28T14:24:49.539812Z"
    },
    "tags": []
   },
   "source": [
    "Three different scenarios were tested.\n",
    "\n",
    "#### 1. Centralized Learning (one model per station)\n",
    "\n",
    "- **average test accuracy  of 70.05%** and standard deviation of 0.0015 (average of accuracy over each station from \\[69%, 73%\\])\n",
    "- precision, recall, f1-score are also close to 70% for all models.\n",
    "- the most misclassified examples belong to the medium class.\n",
    "\n",
    "#### 2. Centralized learning (global model over all stations)\n",
    "\n",
    "- **average test accuracy of 72.4%** with standard deviation of 0.005 (5-fold cross validation)\n",
    "- as in the previous scenario, the samples labeled with medium are misclassified more often than the others. \n",
    "\n",
    "#### 3. Federated Learning (Client == Station)\n",
    "\n",
    "- **average test accuracy of 67.0%** with a standard deviation of 0.014 (5-fold cross validation). \n",
    "- precision, recall and F1-score are around 67%, each with a standard deviation of 0.013. \n",
    "- In a first attempt, without the encoded wind direction and the time features (year, month, day) only a significantly lower accuracy of 63.5% (with a standard deviation of 0.01) was achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845a8bb-b75d-4296-8ca6-3065c22cd49b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Appendix\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfc6d4-e139-48ae-b169-623a0f50b8ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### References\n",
    "<a name=\"cite_note-1\"></a>[(Stock et al., 2023)](#cite_ref-1)  &emsp;  Stock, Petersen, Federrath (2023). *On the Applicability of Federated Learning for Official Statistics*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec50e2-84c5-443e-948a-5668df6f5e81",
   "metadata": {},
   "source": [
    "### Helpful Links\n",
    "- [nbviewer](https://nbviewer.org/https://nbviewer.org/) (correct rendering directly from github)\n",
    "- [Footnotes in Markdown (Stackoverflow)](https://stackoverflow.com/questions/61139741/footnotes-in-markdown-both-on-jupyter-and-google-colab)\n",
    "- [Add Spaces in Markdown (Stackoverflow)](https://stackoverflow.com/questions/47061626/how-to-get-tab-space-in-markdown-cell-of-jupyter-notebook)\n",
    "- [Jupyter Formatting (medium)](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fdhttps://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
