{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5721639",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Olhaau/fl-official-statistics-addon/blob/main/_dev/00_initial_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c22209-c976-434a-b3e7-e6d7f7d5389e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T11:57:47.295804Z",
     "iopub.status.busy": "2023-03-28T11:57:47.295804Z",
     "iopub.status.idle": "2023-03-28T11:57:47.299339Z",
     "shell.execute_reply": "2023-03-28T11:57:47.299339Z",
     "shell.execute_reply.started": "2023-03-28T11:57:47.295804Z"
    },
    "id": "83c22209-c976-434a-b3e7-e6d7f7d5389e",
    "tags": []
   },
   "source": [
    "# Federated Learning in Official Statistics - Initial Results\n",
    "---\n",
    "\n",
    "This notebook presents the **initial results of the originating work** <a name=\"cite_ref-1\"></a>[(Stock et al., 2023)](#cite_note-1).\n",
    "\n",
    "The associated code can be found in the folder 'original_work' of this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc4629-1a7d-44bb-97b6-3b218f91d0d4",
   "metadata": {
    "id": "f0cc4629-1a7d-44bb-97b6-3b218f91d0d4",
    "tags": []
   },
   "source": [
    "## Object of Investigation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7a608-cd8d-47c2-b4d7-7718354abb88",
   "metadata": {
    "id": "caf7a608-cd8d-47c2-b4d7-7718354abb88"
   },
   "source": [
    "To analyze the potential of FL for official statistics, <a name=\"cite_ref-1\"></a>[(Stock et al., 2023)](#cite_note-1) run three simulations with different datasets:\n",
    "\n",
    "1. **Medical Insurance** (presumably artificial): available at [kaggle/ushealthinsurancedataset](https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset).\n",
    "2. **LTE** (mobile radio): privately held by the company [umlaut](https://www.umlaut.com), not publically available.\n",
    "3. **Pollution** (of fine dust PM<sub>2.5</sub> in Bejing): Beijing Multi-Site Air-Quality Data Data Set available at [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02dab58-1acb-4138-aca0-647d9fff46ed",
   "metadata": {
    "id": "a02dab58-1acb-4138-aca0-647d9fff46ed",
    "tags": []
   },
   "source": [
    "## Medical Insurance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ad73a-e6f3-4a1d-ab00-ec84fbabaca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T13:11:16.906742Z",
     "iopub.status.busy": "2023-03-28T13:11:16.906742Z",
     "iopub.status.idle": "2023-03-28T13:11:16.926752Z",
     "shell.execute_reply": "2023-03-28T13:11:16.925751Z",
     "shell.execute_reply.started": "2023-03-28T13:11:16.906742Z"
    },
    "id": "1f7ad73a-e6f3-4a1d-ab00-ec84fbabaca4",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Remark:</b>  A minimal and lightweight use case that addresses a real privacy problem related to official statistics, and identifies opportunities for improving performance in the decentralized setting. This use case is being used to initialize a Federated Learning infrastructure.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b524f7-fc2c-4f0f-9049-b7702eaaa70c",
   "metadata": {
    "id": "e1b524f7-fc2c-4f0f-9049-b7702eaaa70c"
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d284f-7efe-4792-9572-d30b71e23ed9",
   "metadata": {
    "id": "e05d284f-7efe-4792-9572-d30b71e23ed9"
   },
   "source": [
    "Data about medical insurance costs of individual persons.\n",
    "\n",
    "- 1338 records\n",
    "- 8 attributes: age, gender, bmi, children, smoker, region, charges\n",
    "- presumably artificial: complete (no missings), balanced in age, gender \n",
    "\n",
    "As the ML task, the following regression problem is investigated: \n",
    "> *Given the other attributes of the data set, how high are the insurance charges of an individual?*\n",
    "\n",
    "Originating Code can be found in \n",
    "\n",
    "- [med-insurance.ipynb](../original_work/med-insurance/med-insurance.ipynb)\n",
    "- [med-insurance-federated.ipynb](../original_work/med-insurance/med-insurance-federated.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a83b79-4e51-4410-be56-a98a091e7c85",
   "metadata": {
    "id": "e0a83b79-4e51-4410-be56-a98a091e7c85"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Minimal: Scaling + Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b76fe-cceb-4e89-af62-1a1ba4d9003f",
   "metadata": {
    "id": "484b76fe-cceb-4e89-af62-1a1ba4d9003f"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a45958-7259-4290-aa89-fbfcf86bb695",
   "metadata": {
    "id": "58a45958-7259-4290-aa89-fbfcf86bb695"
   },
   "source": [
    "> *The two centralized learning approaches random forest and neural network are used as benchmarks for the FL scenario. A hyperparameter search yields the hyperparameters \\[...\\]. For the second benchmark (neural network), a hyperparameter search yields the following architecture: 16 dense-neurons in the first layer and 2 dense-neurons in the second layer, followed by a single dense-neuron in the output layer. The neural network is trained for 100 epochs, using the stochastic gradient descent (SGD) implementation by TensorFlow.* \n",
    "\n",
    "> *For the FL scenario, we use a slightly larger neural network with 16 dense-neurons in the first layer and 6 neurons in the second layer, again followed by a single neuron in the output layer. We run the FL training process for 150 rounds, with SGD and a learning rate of 0.8 for the clients and 3.0 for the server.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53523d90-6554-4e49-a98d-7123a0b1d89f",
   "metadata": {},
   "source": [
    "#### Our Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1a0d9-d74e-4dcb-96f7-99fe3a6cf49a",
   "metadata": {},
   "source": [
    "tba: Rem.: Differences in the Models, Tuning not suitable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c5317-b3ee-4def-9dfc-0b6aa4380ce7",
   "metadata": {
    "id": "037c5317-b3ee-4def-9dfc-0b6aa4380ce7"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525624c-8dc8-4bf9-a902-8cc32134fb07",
   "metadata": {
    "id": "3525624c-8dc8-4bf9-a902-8cc32134fb07"
   },
   "source": [
    "Performance is measured by a $R^2$ score of a test set (holdout evaluation).\n",
    "\n",
    "- Benchmarks (centralized)\n",
    "    - 0.877 random forest regressor\n",
    "    - 0.85 neural network\n",
    "- Federated Learning\n",
    "    - -0.075"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6abd95-5f2c-48ff-b6d1-7cf251115e3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665d395-5256-4761-8866-b5150d600542",
   "metadata": {},
   "source": [
    "Cf. <a name=\"cite_ref-1\"></a>[(Stock et al., 2023, p. 4)](#cite_note-1):\n",
    "\n",
    "> *The strong performance of the centralized benchmark models (R2 of 0.877 and 0.85) may be due to the ML-friendly and possibly artificial nature of the data set. Regarding the significantly worse performance of the FL neural network (R2 of -0.075), we test the hypothesis that this is rooted in the fact that the FL training data is lacking the region attribute. Recall that we construct the FL client data sets by dividing the data set by its region. Without removing the attribute from the data, the attribute would hence be the same in each client data set – thus it has no use in the local training process. To test the influence of the region attribute on the model performance, we retrain the benchmark random forest regressor on a copy of the original data set without the one-hot encoded region attribute. This does not influence the performance of the benchmark, as it still achieves an R2 of 0.877. We run a second test in the FL scenario: Instead of dividing the data set by the region of the data records, we randomly split the records into 4 evenly sized client data sets and keep the region attribute. If the hypothesis above (that the region attribute contains useful information for the training process, explaining the strong performance of the benchmark models) is true, we would expect a big improvement of the FL model by keeping the region attribute in the (randomly dispersed) client data sets. However, the original result of -0.075 R2 (with one client per region) has been worsened in this test scenario (with random client data sets) to an R2 value of -0.106. Thus, we conclude that we can find no evidence for the hypothesis stated above.* \n",
    ">\n",
    "> *Instead, we conjecture that the data set is too small for a FL training scenario – with about 350 data records per client, minus 20% for the test data. Although data augmentation (adding more artificial data with a similar distribution) could be a possible solution to this problem, we want to stress that this is not trivial. In a quick test, in which we have used a generative adversarial network (GAN), we were not able to improve the resulting FL regressor’s performance by data augmentation, reaching an R2 value of -1.02.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5a719-ab55-46a5-a14d-ef7fa9b56ffd",
   "metadata": {},
   "source": [
    "#### Our Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbb7c0-6dcf-4996-b071-510bb9235463",
   "metadata": {},
   "source": [
    "We had Problems to understand the low performance of FL (or improve it). So we took a closer look at the centralized approach and noticed high Variance in the training of the neural networks.\n",
    "\n",
    "*Training Performance with initial parameters:*\n",
    "![](../original_work/med-insurance/rsquared_init_params.jpg)\n",
    "\n",
    "*Training Performance after tuning:*\n",
    "![](../original_work/med-insurance/rsquared_hyperparams.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e7f05-5a87-4a38-8d94-31d4be39d03d",
   "metadata": {
    "id": "d01e7f05-5a87-4a38-8d94-31d4be39d03d",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## LTE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111fed2-b859-4c4b-9bf0-34d17df3f53b",
   "metadata": {
    "id": "6111fed2-b859-4c4b-9bf0-34d17df3f53b"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Remark</b> The use case is currently not being investigated further because the data is not available. Any further investigation would require umlaut's participation to access the data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d65b9e-e902-45be-8d72-158974afa8a6",
   "metadata": {
    "id": "16d65b9e-e902-45be-8d72-158974afa8a6"
   },
   "source": [
    "more tba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b998b9-9caa-485f-98f7-e1f530a2809a",
   "metadata": {
    "id": "90b998b9-9caa-485f-98f7-e1f530a2809a"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e395f2-b322-43e2-8504-f47cf90a2467",
   "metadata": {
    "id": "e6e395f2-b322-43e2-8504-f47cf90a2467"
   },
   "source": [
    "Results The benchmarks of the centralized learning regressors are \n",
    "\n",
    "- $R^2 = 0.158$ (random forest)\n",
    "- $R^2 = 0.13$ (neural network)\n",
    "- $R^2 = 0.13$ (linear regression) \n",
    "- $R^2 = 0.114$ (neural network - Federated Learning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb6481-4d64-4874-9049-44f507ebfe7c",
   "metadata": {
    "id": "7fbb6481-4d64-4874-9049-44f507ebfe7c",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Pollution\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5534382-7c4e-4bae-8718-36d065a11247",
   "metadata": {
    "id": "f5534382-7c4e-4bae-8718-36d065a11247"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Remark</b> Very good performance, but no real privacy issues. A good use case for upcomming technical tests, but no suitable product for official statistics and to present the advantages of Federated Learning.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b590de7-7381-4d02-abe8-8dc2eb4ac233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:32:55.478388Z",
     "iopub.status.busy": "2023-03-28T14:32:55.478388Z",
     "iopub.status.idle": "2023-03-28T14:32:55.491135Z",
     "shell.execute_reply": "2023-03-28T14:32:55.491135Z",
     "shell.execute_reply.started": "2023-03-28T14:32:55.478388Z"
    },
    "id": "0b590de7-7381-4d02-abe8-8dc2eb4ac233",
    "tags": []
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275478c-b2c1-4ba7-88b7-3ec0c482cec5",
   "metadata": {
    "id": "3275478c-b2c1-4ba7-88b7-3ec0c482cec5",
    "tags": []
   },
   "source": [
    "> *We model a classification task in which the current fine dust pollution is inferred based on meteorological input data. More precisely, 48 consecutive hourly measurements are used to make a prediction for the current PM2.5 pollution (the total weight of particles smaller than 2.5 μm in one m3). The output of the predictor is one of the three classes low, medium or high. The threshold for each class are chosen in a way such that the samples of the whole data set are distributed evenly among the three classes.* \n",
    "\n",
    "#### Dataset\n",
    "\n",
    "> *The data set we use is a multi-feature air quality and weather data set. It consists of hourly measurements of 12 meteorological stations in Beijing, recorded over a time span of 4 years (2013–2017). In total, more than 420 000 data records are included in the data set. Although some attributes are missing for some data records, most records have data for all of a total of 17 attributes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb348c0-96b5-4611-8a74-9aafa9c13dd6",
   "metadata": {
    "id": "eeb348c0-96b5-4611-8a74-9aafa9c13dd6"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51acc2-e4ce-4065-8cab-d65d63e5f36a",
   "metadata": {
    "id": "0f51acc2-e4ce-4065-8cab-d65d63e5f36a"
   },
   "source": [
    "> *To complete the missing data records, we use linear interpolation. We encode the wind direction by parsing the wd attribute into four binary attributes (one for each cardinal direction). All other features are scaled using a standard scaler implementation. We exclude the following pollution features from training, since we expect a high correlation with the target attribute PM2.5: PM10, SO2, NO2, CO and O3.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf9d26-28b7-4985-886a-681ebd719cd2",
   "metadata": {
    "id": "16cf9d26-28b7-4985-886a-681ebd719cd2"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084ba61-fb8d-4932-b16c-543ca720e78e",
   "metadata": {
    "id": "e084ba61-fb8d-4932-b16c-543ca720e78e"
   },
   "source": [
    "> *We use the same model architecture for all three scenarios, a neural network with five layers: A 10-neuron LSTM (long-short term memory) layer, a dropout layer with a dropout rate of 25%, a 5-neuron LSTM layer, another dropout layer with a dropout rate of 35% and a 3-neuron dense layer for the classification output.*\n",
    "\n",
    "> *We train for 20 epochs in the first scenario, 10 epochs in the second scenario and 160 epochs in third scenario (FL). In all scenarios, we use CategoricalCrossEntropy as the loss function. While we use the Adam optimizer with an automatic learning rate in both of the centralized learning scenarios, we employ the Stochastic Gradient Descent (SGD) optimizer in the FL scenario. On the server we use a learning rate of 1 for SGD, on the client we start with a learning rate of 0.1. The latter is divided by 10 every 64 rounds, such that at the end of the 160 epochs in the FL scenario, the client learning rate is at 0.001.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05606ddf-fb0e-4c06-b5c1-1203ed6e2a9b",
   "metadata": {
    "id": "05606ddf-fb0e-4c06-b5c1-1203ed6e2a9b"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3d293-3594-4e36-a4d3-133bd92f6e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:24:49.539812Z",
     "iopub.status.busy": "2023-03-28T14:24:49.538811Z",
     "iopub.status.idle": "2023-03-28T14:24:49.563681Z",
     "shell.execute_reply": "2023-03-28T14:24:49.562635Z",
     "shell.execute_reply.started": "2023-03-28T14:24:49.539812Z"
    },
    "id": "06b3d293-3594-4e36-a4d3-133bd92f6e5e",
    "tags": []
   },
   "source": [
    "Three different scenarios were tested.\n",
    "\n",
    "#### 1. Centralized Learning (one model per station)\n",
    "\n",
    "- **average test accuracy  of 70.05%** and standard deviation of 0.0015 (average of accuracy over each station from \\[69%, 73%\\])\n",
    "- precision, recall, f1-score are also close to 70% for all models.\n",
    "- the most misclassified examples belong to the medium class.\n",
    "\n",
    "#### 2. Centralized learning (global model over all stations)\n",
    "\n",
    "- **average test accuracy of 72.4%** with standard deviation of 0.005 (5-fold cross validation)\n",
    "- as in the previous scenario, the samples labeled with medium are misclassified more often than the others. \n",
    "\n",
    "#### 3. Federated Learning (Client == Station)\n",
    "\n",
    "- **average test accuracy of 67.0%** with a standard deviation of 0.014 (5-fold cross validation). \n",
    "- precision, recall and F1-score are around 67%, each with a standard deviation of 0.013. \n",
    "- In a first attempt, without the encoded wind direction and the time features (year, month, day) only a significantly lower accuracy of 63.5% (with a standard deviation of 0.01) was achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845a8bb-b75d-4296-8ca6-3065c22cd49b",
   "metadata": {
    "id": "2845a8bb-b75d-4296-8ca6-3065c22cd49b",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Appendix\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfc6d4-e139-48ae-b169-623a0f50b8ed",
   "metadata": {
    "id": "16bfc6d4-e139-48ae-b169-623a0f50b8ed",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### References\n",
    "<a name=\"cite_note-1\"></a>[(Stock et al., 2023)](#cite_ref-1)  &emsp;  Stock, Petersen, Federrath (2023). *On the Applicability of Federated Learning for Official Statistics*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec50e2-84c5-443e-948a-5668df6f5e81",
   "metadata": {
    "id": "aaec50e2-84c5-443e-948a-5668df6f5e81"
   },
   "source": [
    "### Helpful Links\n",
    "- [nbviewer](https://nbviewer.org/https://nbviewer.org/) (correct rendering directly from github)\n",
    "- [Footnotes in Markdown (Stackoverflow)](https://stackoverflow.com/questions/61139741/footnotes-in-markdown-both-on-jupyter-and-google-colab)\n",
    "- [Add Spaces in Markdown (Stackoverflow)](https://stackoverflow.com/questions/47061626/how-to-get-tab-space-in-markdown-cell-of-jupyter-notebook)\n",
    "- [Jupyter Formatting (medium)](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fdhttps://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
