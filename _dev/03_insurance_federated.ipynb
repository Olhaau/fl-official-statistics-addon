{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olhaau/fl-official-statistics-addon/blob/main/_dev/03_insurance_federated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1N2mtyx2Uxs"
      },
      "source": [
        "# Medical Insurance - a Federated Learning Use Case.\n",
        "\n",
        "This notebook contains Federated Learning.\n",
        "\n",
        "-- **more tba** --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qdJnO4M7VDO"
      },
      "source": [
        "## Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-28T14:27:44.075055Z",
          "iopub.status.busy": "2023-03-28T14:27:44.075055Z",
          "iopub.status.idle": "2023-03-28T14:27:44.092127Z",
          "shell.execute_reply": "2023-03-28T14:27:44.092127Z",
          "shell.execute_reply.started": "2023-03-28T14:27:44.075055Z"
        },
        "tags": [],
        "id": "urYRSTDKV8Nl"
      },
      "outputs": [],
      "source": [
        "# Is a repo-clone and installs needed (e.g. in colabs)? \n",
        "need_clone_install = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZQgDGzbd3pm"
      },
      "source": [
        "### Pull Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-28T11:35:39.951921Z",
          "iopub.status.busy": "2023-03-28T11:35:39.950864Z",
          "iopub.status.idle": "2023-03-28T11:36:02.975338Z",
          "shell.execute_reply": "2023-03-28T11:36:02.974338Z",
          "shell.execute_reply.started": "2023-03-28T11:35:39.951921Z"
        },
        "id": "F5qBP638d0iR",
        "outputId": "fa117f09-6cd7-4fa5-9cd7-49a68233f38d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fl-official-statistics-addon'...\n",
            "remote: Enumerating objects: 794, done.\u001b[K\n",
            "remote: Counting objects: 100% (430/430), done.\u001b[K\n",
            "remote: Compressing objects: 100% (263/263), done.\u001b[K\n",
            "remote: Total 794 (delta 216), reused 355 (delta 161), pack-reused 364\u001b[K\n",
            "Receiving objects: 100% (794/794), 36.98 MiB | 11.92 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n",
            "Updating files: 100% (292/292), done.\n",
            "/content/fl-official-statistics-addon\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "if need_clone_install:\n",
        "    import os\n",
        "    \n",
        "    # rm repo from gdrive\n",
        "    if os.path.exists(\"fl-official-statistics-addon\"):\n",
        "      %rm -r fl-official-statistics-addon\n",
        "\n",
        "    # clone\n",
        "    !git clone https://github.com/Olhaau/fl-official-statistics-addon\n",
        "    %cd fl-official-statistics-addon\n",
        "\n",
        "    # pull (the currenct version of the repo)\n",
        "    !git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5TMrK67Xxv"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UQSYlSd47WaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d13d4e8-b5ee-4ec8-94bf-d2b75c381c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 KB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.0/349.0 KB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 KB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 KB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.8/39.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 KB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.1.2 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "orbax 0.1.6 requires jax>=0.4.6, but you have jax 0.3.15 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker~=1.3.1, but you have portpicker 1.5.2 which is incompatible.\n",
            "flax 0.6.7 requires jax>=0.4.2, but you have jax 0.3.15 which is incompatible.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "if need_clone_install: \n",
        "  !pip install --quiet nest_asyncio\n",
        "  !pip install --quiet tensorflow_federated\n",
        "  !pip install --quiet tensorflow_addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUcRlUPV7bZi"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2zMmbNFf7Wc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d170081-36be-41f3-ca56-39be66df352c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow_addons.metrics import RSquare\n",
        "import tensorflow_addons as tfa\n",
        "import nest_asyncio\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxFNFscq8O74"
      },
      "source": [
        "## Ingest and Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "C9zyHqiN9fCo",
        "outputId": "827de64a-7cf5-46c7-dcb4-0934c757e33c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        age  sex       bmi  children  smoker     region      charges  region0  \\\n",
              "0  0.021739  0.0  0.321227       0.0     1.0  southwest  16884.92400      0.0   \n",
              "1  0.000000  1.0  0.479150       0.2     0.0  southeast   1725.55230      0.0   \n",
              "2  0.217391  1.0  0.458434       0.6     0.0  southeast   4449.46200      0.0   \n",
              "3  0.326087  1.0  0.181464       0.0     0.0  northwest  21984.47061      0.0   \n",
              "4  0.304348  1.0  0.347592       0.0     0.0  northwest   3866.85520      0.0   \n",
              "\n",
              "   region1  region2  region3  \n",
              "0      0.0      0.0      1.0  \n",
              "1      0.0      1.0      0.0  \n",
              "2      0.0      1.0      0.0  \n",
              "3      1.0      0.0      0.0  \n",
              "4      1.0      0.0      0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48e750f0-b2eb-4a7a-a4b6-d5e9d3d04bad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "      <th>region0</th>\n",
              "      <th>region1</th>\n",
              "      <th>region2</th>\n",
              "      <th>region3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.321227</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.479150</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.217391</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.458434</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.326087</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.181464</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.304348</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.347592</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48e750f0-b2eb-4a7a-a4b6-d5e9d3d04bad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48e750f0-b2eb-4a7a-a4b6-d5e9d3d04bad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48e750f0-b2eb-4a7a-a4b6-d5e9d3d04bad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df = pd.read_csv(\"output/data/insurance-clean.csv\", index_col = 0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qam6babo7Wgg"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 4\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 10\n",
        "SHUFFLE_BUFFER = 20\n",
        "PREFETCH_BUFFER = 5\n",
        "NUM_ROUNDS = 150\n",
        "RUN_NAME = f'0,8-3({NUM_ROUNDS})-{NUM_EPOCHS}-epochs-{BATCH_SIZE}-batch-WithRegion/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JTutbOjq8VzQ"
      },
      "outputs": [],
      "source": [
        "syn_samples_per_region = 1000\n",
        "\n",
        "def get_dataset_for_region(dataset, region_index, test_size_per_region=20):\n",
        "    \"\"\"Min-max scale and return data for a single, given region. The scaler must be fitted before.\n",
        "\n",
        "    :param dataset: The dataset to get the regional data from\n",
        "    :type dataset: pandas.DataFrame\n",
        "    :param region_index: The index number of the region to return\n",
        "    :type region_index: int\n",
        "    :param test_size_per_region: The amount of values to separate for testing, default are 20\n",
        "    :type test_size_per_region: int, optional\n",
        "    :return: The dataset specific for the defined region, the test values, the test labels\n",
        "    :rtype: tensorflow.python.data.ops.dataset_ops.PrefetchDataset, tuple of pandas.core.series.Series\n",
        "    \"\"\"\n",
        "    region_ds = dataset[dataset['region'] == region_index]\n",
        "    region_ds = region_ds.drop(columns=['region'])\n",
        "    len = region_ds.shape[0]\n",
        "\n",
        "    # The scaling into [0, 1] is not necessary anymore, it happens when the data loads already\n",
        "    # region_ds[['age', 'bmi', 'children']] = scaler.transform(region_ds[['age', 'bmi', 'children']])\n",
        "\n",
        "    X_test = region_ds.head(test_size_per_region)\n",
        "    y_test = X_test.pop('charges')\n",
        "\n",
        "    X_train = region_ds.tail(len - test_size_per_region)\n",
        "    y_train = X_train.pop('charges')\n",
        "\n",
        "    fed_train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
        "\n",
        "    return (\n",
        "        fed_train_dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER),\n",
        "        (X_test, y_test)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8FEwLlGv8eq3"
      },
      "outputs": [],
      "source": [
        "# Create test and train sets and put them into random_client_ds, use four clients which are independent of the region\n",
        "def get_dataset_random_region(dataset, num_clients=4, test_size_per_region=20):\n",
        "    \"\"\"Creates a list with client datasets independent of the region.\n",
        "\n",
        "    :param dataset: The dataset to get the regional data from\n",
        "    :type dataset: pandas.DataFrame\n",
        "    :param num_clients: the number of clients create (equal big datasets per client), default value is 4 clients\n",
        "    :type num_clients: int, optional\n",
        "    :param test_size_per_region: The amount of values to separate for testing, default are 20\n",
        "    :type test_size_per_region: int, optional\n",
        "    :return: List of the prepared dataset with one entry per region, the test values and labels for each region\n",
        "    :rtype: List of (tensorflow.python.data.ops.dataset_ops.PrefetchDataset, tuple of pandas.core.series.Series)\"\"\"\n",
        "    size_of_client_ds = int(dataset.shape[0] / num_clients)\n",
        "\n",
        "    dataset_to_split = dataset.copy()\n",
        "    dataset_to_split.pop(\"region\")\n",
        "    random_client_ds = []\n",
        "    for i in range(num_clients):\n",
        "        sampled = dataset_to_split.sample(n=size_of_client_ds)\n",
        "        dataset_to_split.drop(sampled.index)\n",
        "\n",
        "        X_test = sampled.head(test_size_per_region)\n",
        "        y_test = X_test.pop('charges')\n",
        "\n",
        "        X_train = sampled.tail(size_of_client_ds - test_size_per_region)\n",
        "        y_train = X_train.pop('charges')\n",
        "\n",
        "        fed_train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
        "\n",
        "        train_set = fed_train_dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)\n",
        "        test_set = (X_test, y_test)\n",
        "\n",
        "        random_client_ds.append((train_set, test_set))\n",
        "\n",
        "    return random_client_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dirvrz6Y9ARK",
        "outputId": "54cd90e6-62e3-4a95-a0d1-85c35c886b6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 5), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
              "  (Empty DataFrame\n",
              "   Columns: [age, sex, bmi, children, smoker]\n",
              "   Index: [], Series([], Name: charges, dtype: float64))),\n",
              " (<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 5), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
              "  (Empty DataFrame\n",
              "   Columns: [age, sex, bmi, children, smoker]\n",
              "   Index: [], Series([], Name: charges, dtype: float64))),\n",
              " (<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 5), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
              "  (Empty DataFrame\n",
              "   Columns: [age, sex, bmi, children, smoker]\n",
              "   Index: [], Series([], Name: charges, dtype: float64)))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Training data for clients with regional data (each client one region)\n",
        "test_size_per_region = 20\n",
        "regions = ['region0', 'region1', 'region2', 'region3']\n",
        "\n",
        "federated_insurance_data = [\n",
        "    get_dataset_for_region(df.drop(regions, axis=1), i, test_size_per_region=test_size_per_region)\n",
        "    for i in range(NUM_CLIENTS-1)]\n",
        "federated_insurance_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mX6HRriR9Cil"
      },
      "outputs": [],
      "source": [
        "# Training data for clients with regional independent data\n",
        "random_client_ds = get_dataset_random_region(df, num_clients=NUM_CLIENTS, test_size_per_region=test_size_per_region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiNQ6m66A7sa"
      },
      "source": [
        "### Code Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRbSvR-Q9058",
        "outputId": "74ef265b-1052-46b0-e3e2-3b5388426d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "splitted datasets: 3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print(\"splitted datasets: {}\".format(len(federated_insurance_data)))\n",
        "print(\"------------------------------\")\n",
        "# Q: why range(NUM_CLIENTS-1) and, thus, only 3 clients?\n",
        "\n",
        "# Q: how to output? -> does not work or nothing in it?\n",
        "# -> i think its empty\n",
        "list(federated_insurance_data[0][0].as_numpy_iterator())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJY8_tVdCXrz"
      },
      "source": [
        "#### Show Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFohPc1rAqVw",
        "outputId": "1f9ace80-703e-43a8-e4f2-3fa458644107"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Create a Tensor Object\n",
        "X_train = df.iloc[:,0:4]\n",
        "y_train = df['charges']\n",
        "df2 = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpW57hM4DOBI",
        "outputId": "82870ca0-0260-4c16-ce9f-be632b2ca3b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([0.02173913, 0.        , 0.3212268 , 0.        ]), 16884.924),\n",
              " (array([0.        , 1.        , 0.47914985, 0.2       ]), 1725.5523),\n",
              " (array([0.2173913 , 1.        , 0.45843422, 0.6       ]), 4449.462),\n",
              " (array([0.32608696, 1.        , 0.18146355, 0.        ]), 21984.47061),\n",
              " (array([0.30434783, 1.        , 0.34759214, 0.        ]), 3866.8552),\n",
              " (array([0.2826087 , 0.        , 0.26311542, 0.        ]), 3756.6216),\n",
              " (array([0.60869565, 0.        , 0.47027172, 0.2       ]), 8240.5896),\n",
              " (array([0.41304348, 0.        , 0.31692225, 0.6       ]), 7281.5056),\n",
              " (array([0.41304348, 1.        , 0.37315039, 0.4       ]), 6406.4107),\n",
              " (array([0.91304348, 0.        , 0.26580576, 0.        ]), 28923.13692)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Show Tensor Object\n",
        "# S. https://stackoverflow.com/questions/62436302/extract-target-from-tensorflow-prefetchdataset\n",
        "list(df2.as_numpy_iterator())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvPyDQL8iXm"
      },
      "source": [
        "## Model Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9Csom8dD8hRK"
      },
      "outputs": [],
      "source": [
        "def create_keras_model(\n",
        "    input_features = 9,\n",
        "    initializer = 'zeros',\n",
        "    units = [16,6,1],\n",
        "    activation = None\n",
        "    ):\n",
        "    \"\"\"Create neural network for given number of input features)\n",
        "\n",
        "    :param input_features: The dimension of the first layer with represents the amount of features used\n",
        "    :type input_features: int, optional\n",
        "    :return: Created but not compiled model\n",
        "    :rtype: keras.Model\n",
        "    \"\"\"\n",
        "    return tf.keras.models.Sequential([\n",
        "        # without region: tf.keras.layers.InputLayer(input_shape=(5,)),\n",
        "        tf.keras.layers.InputLayer(input_shape=(input_features,)),\n",
        "        tf.keras.layers.Dense(units[0], kernel_initializer=initializer, activation = activation\n",
        "                              ),\n",
        "        tf.keras.layers.Dense(units[1], kernel_initializer=initializer, activation = activation\n",
        "                              ),\n",
        "        tf.keras.layers.Dense(units[2], kernel_initializer=initializer)\n",
        "    ])\n",
        "\n",
        "\n",
        "# A helper function for federated learning\n",
        "def model_fn(    \n",
        "    input_features = 9,\n",
        "    initializer = 'zeros',\n",
        "    units = [16,6,1],\n",
        "    activation = None):\n",
        "    \"\"\"A function for TFF to create a local model during federated learning and return it as the correct type.\n",
        "    This model uses 9 input features i.e. the regions are part of the features.\n",
        "\n",
        "    :return: The LSTM model to be used as federated models\n",
        "    :rtype: tff.learning.Model\n",
        "    \"\"\"\n",
        "    # We _must_ create a new model here, and _not_ capture it from an external\n",
        "    # scope. TFF will call this within different graph contexts.\n",
        "    keras_model = create_keras_model(\n",
        "            input_features = input_features,\n",
        "            initializer = initializer,\n",
        "            activation = activation,\n",
        "            units = units\n",
        "    )\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        # without region: \n",
        "        #input_spec = federated_insurance_data[0][0].element_spec,\n",
        "        input_spec = random_client_ds[0][0].element_spec,\n",
        "        loss = tf.keras.losses.MeanAbsoluteError(),\n",
        "        metrics = [tf.keras.metrics.MeanAbsoluteError()\n",
        "        #,tfa.metrics.RSquare()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# Helper functions for different features as input\n",
        "def model_fn_5():\n",
        "    return model_fn(5)\n",
        "\n",
        "def model_fn_5_mod():\n",
        "  return model_fn(5, initializer = 'glorot_uniform', activation = 'relu')\n",
        "\n",
        "def model_fn_9():\n",
        "  return model_fn(9)\n",
        "\n",
        "def model_fn_9_mod():\n",
        "  return model_fn(9, initializer = 'glorot_uniform', activation = 'relu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sc8xDdk2RTp"
      },
      "source": [
        "## Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GGvMq3ATosV"
      },
      "source": [
        "### Original Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgodbIMsH2b9"
      },
      "source": [
        "#### Create Learning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K3k21nSQ9P-D"
      },
      "outputs": [],
      "source": [
        "# Create iterative learning process which will perform the federated learning\n",
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn_9,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.8),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBMUBG1tHFfC",
        "outputId": "f702596c-c0dd-4386-cb52-3be5c1833ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( -> <\n",
            "  global_model_weights=<\n",
            "    trainable=<\n",
            "      float32[9,16],\n",
            "      float32[16],\n",
            "      float32[16,6],\n",
            "      float32[6],\n",
            "      float32[6,1],\n",
            "      float32[1]\n",
            "    >,\n",
            "    non_trainable=<>\n",
            "  >,\n",
            "  distributor=<>,\n",
            "  client_work=<>,\n",
            "  aggregator=<\n",
            "    value_sum_process=<>,\n",
            "    weight_sum_process=<>\n",
            "  >,\n",
            "  finalizer=<\n",
            "    int64,\n",
            "    float32[9,16],\n",
            "    float32[16],\n",
            "    float32[16,6],\n",
            "    float32[6],\n",
            "    float32[6,1],\n",
            "    float32[1]\n",
            "  >\n",
            ">@SERVER)\n"
          ]
        }
      ],
      "source": [
        "# The initial setup of the learning process\n",
        "print(iterative_process.initialize.type_signature.formatted_representation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A-Lx1oGH9Jy"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kuAoyOYNbyr5"
      },
      "outputs": [],
      "source": [
        "state = iterative_process.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSqO7EPIHQXJ",
        "outputId": "fe844044-3c46-4a3b-e582-29e38222fb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [01:27<00:00,  1.69it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train the federated model with random clients\n",
        "for round_num in tqdm.tqdm(range(1, NUM_ROUNDS)):\n",
        "    result = iterative_process.next(state, [f[0] for f in random_client_ds])\n",
        "    state = result.state\n",
        "    metrics = result.metrics\n",
        "    for name, value in metrics['client_work']['train'].items():\n",
        "        tf.summary.scalar(name, value, step=round_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Kpx2vpWhQY",
        "outputId": "930e0fef-a3f5-4235-fcce-a66b4b1236dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('distributor', ()),\n",
              "             ('client_work',\n",
              "              OrderedDict([('train',\n",
              "                            OrderedDict([('mean_absolute_error', 8636.581),\n",
              "                                         ('loss', 8636.581),\n",
              "                                         ('num_examples', 6280),\n",
              "                                         ('num_batches', 628)]))])),\n",
              "             ('aggregator',\n",
              "              OrderedDict([('mean_value', ()), ('mean_weight', ())])),\n",
              "             ('finalizer', OrderedDict([('update_non_finite', 0)]))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYfnkG4T1qf",
        "outputId": "5ba066c6-135e-49c6-dc6b-bfef388d1774"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LearningAlgorithmState(global_model_weights=ModelWeights(trainable=[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32), array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0.], dtype=float32), array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]], dtype=float32), array([9554.318], dtype=float32)], non_trainable=[]), distributor=(), client_work=(), aggregator=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), finalizer=[149, array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32), array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]], dtype=float32), array([-0., -0., -0., -0., -0., -0.], dtype=float32), array([[-0.],\n",
              "       [-0.],\n",
              "       [-0.],\n",
              "       [-0.],\n",
              "       [-0.],\n",
              "       [-0.]], dtype=float32), array([0.60058594], dtype=float32)])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "result.state\n",
        "# all weights are zero\n",
        "# -> nothing is learned "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqkadG5nH_KU"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XwZd8b3FHjuh"
      },
      "outputs": [],
      "source": [
        "# Create the test data for model evaluation\n",
        "X_test = pd.concat([f[1][0] for f in random_client_ds])\n",
        "y_test = pd.concat([f[1][1] for f in random_client_ds])\n",
        "\n",
        "test_sets = [tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.convert_to_tensor(np.expand_dims(el[1][0], axis=0)), \n",
        "    tf.convert_to_tensor(np.expand_dims(el[1][1], axis=0)))) \n",
        "    for el in random_client_ds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl-0NiBSJDuO",
        "outputId": "695ec5bf-b0e2-42cd-d7d0-b4785744ce0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-552d4a23315f>:2: DeprecationWarning: `tff.learning.build_federated_evaluation` is deprecated, use `tff.learning.algorithms.build_fed_eval` instead.\n",
            "  evaluation = tff.learning.build_federated_evaluation(model_fn_9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('eval',\n",
              "              OrderedDict([('mean_absolute_error', 8413.922),\n",
              "                           ('loss', 8413.922),\n",
              "                           ('num_examples', 80),\n",
              "                           ('num_batches', 4)]))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Model evaluation\n",
        "evaluation = tff.learning.build_federated_evaluation(model_fn_9)\n",
        "# print(evaluation.type_signature.formatted_representation())\n",
        "model_weights = iterative_process.get_model_weights(state)\n",
        "train_metrics = evaluation(model_weights, test_sets)\n",
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHtfuogKJe6z",
        "outputId": "c8df4c54-c140-4692-d83e-097ad622b7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 8ms/step - loss: 8413.9229 - mae: 8413.9229 - mean_squared_error: 170049920.0000\n",
            "[8413.9228515625, 8413.9228515625, 170049920.0]\n",
            "['loss', 'mae', 'mean_squared_error']\n"
          ]
        }
      ],
      "source": [
        "# Create model from training results and evaluate\n",
        "model = create_keras_model(input_features = 9)\n",
        "model_weights.assign_weights_to(model)\n",
        "model.compile(\n",
        "    loss=tf.losses.mae,\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    metrics=[\"mae\", 'mean_squared_error']\n",
        ")\n",
        "# The evaluation results, for technical reasons the metrics_names is called afterwards. However, its order fits to the results\n",
        "print(model.evaluate(X_test, y_test))\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-5ZQ6AyUpyX"
      },
      "source": [
        "### Test New Learner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLjpPo1sU3A2"
      },
      "source": [
        "#### Create Learning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FdUZhFGBUv6z"
      },
      "outputs": [],
      "source": [
        "# Create iterative learning process which will perform the federated learning\n",
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn_9_mod,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=.05))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNmoEYFdUv61",
        "outputId": "4c8a9d64-9749-4691-9090-14e96d6c8596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( -> <\n",
            "  global_model_weights=<\n",
            "    trainable=<\n",
            "      float32[9,16],\n",
            "      float32[16],\n",
            "      float32[16,6],\n",
            "      float32[6],\n",
            "      float32[6,1],\n",
            "      float32[1]\n",
            "    >,\n",
            "    non_trainable=<>\n",
            "  >,\n",
            "  distributor=<>,\n",
            "  client_work=<>,\n",
            "  aggregator=<\n",
            "    value_sum_process=<>,\n",
            "    weight_sum_process=<>\n",
            "  >,\n",
            "  finalizer=<\n",
            "    int64,\n",
            "    float32[9,16],\n",
            "    float32[16],\n",
            "    float32[16,6],\n",
            "    float32[6],\n",
            "    float32[6,1],\n",
            "    float32[1]\n",
            "  >\n",
            ">@SERVER)\n"
          ]
        }
      ],
      "source": [
        "# The initial setup of the learning process\n",
        "print(iterative_process.initialize.type_signature.formatted_representation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q8XzfXlU5q5"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ROyd0BlXb2xr"
      },
      "outputs": [],
      "source": [
        "state = iterative_process.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEUgryWFU5q6",
        "outputId": "8e97ec55-9664-4994-c9d6-818a778311d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [01:41<00:00,  1.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train the federated model with random clients\n",
        "for round_num in tqdm.tqdm(range(1, NUM_ROUNDS)):\n",
        "    result = iterative_process.next(state, [f[0] for f in random_client_ds])\n",
        "    state = result.state\n",
        "    metrics = result.metrics\n",
        "    for name, value in metrics['client_work']['train'].items():\n",
        "        tf.summary.scalar(name, value, step=round_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwFehjaXOGp",
        "outputId": "d143f32c-472a-4f89-987d-6456c5943e69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('distributor', ()),\n",
              "             ('client_work',\n",
              "              OrderedDict([('train',\n",
              "                            OrderedDict([('mean_absolute_error', 4011.771),\n",
              "                                         ('loss', 4011.77),\n",
              "                                         ('num_examples', 6280),\n",
              "                                         ('num_batches', 628)]))])),\n",
              "             ('aggregator',\n",
              "              OrderedDict([('mean_value', ()), ('mean_weight', ())])),\n",
              "             ('finalizer', OrderedDict([('update_non_finite', 0)]))])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3opn587U5q8",
        "outputId": "04ea92a8-e78b-4c3c-c652-22137d6a82c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LearningAlgorithmState(global_model_weights=ModelWeights(trainable=[array([[ 5.43178701e+00,  1.97150002e+01, -6.51558712e-02,\n",
              "        -3.99578780e-01,  3.23862171e+00,  6.01310804e-02,\n",
              "         9.53441143e+00,  2.04419708e+01,  5.48346102e-01,\n",
              "         2.27880077e+01,  1.22292332e-01, -1.68645069e-01,\n",
              "        -1.05951816e-01,  1.61645224e-03,  2.64075165e+01,\n",
              "         3.64008832e+00],\n",
              "       [-1.14441335e+00, -1.41532564e+00, -2.66328722e-01,\n",
              "         1.07548404e+00, -6.06195390e-01,  5.81667662e-01,\n",
              "        -1.71022201e+00,  2.11605668e+00,  5.51581144e-01,\n",
              "        -2.17793059e+00, -1.51768908e-01,  3.57665807e-01,\n",
              "         1.89159304e-01, -1.30872831e-01,  1.58753061e+00,\n",
              "        -9.43277717e-01],\n",
              "       [-9.75974441e-01, -1.40539300e+00, -7.58955851e-02,\n",
              "         4.23496094e+01,  1.67166340e+00,  2.19221287e+01,\n",
              "         1.06946325e+00,  4.00061750e+00,  1.86938953e+01,\n",
              "         9.38584089e-01,  2.82787651e-01,  1.34035892e+01,\n",
              "         6.89760828e+00,  2.70180225e-01,  5.97382307e+00,\n",
              "         1.10425198e+00],\n",
              "       [ 5.22437286e+00,  1.14328928e+01, -5.74687719e-02,\n",
              "        -2.00494838e+00,  2.53611493e+00, -8.17666531e-01,\n",
              "         1.99423516e+00, -4.45715427e+00, -8.70833755e-01,\n",
              "         9.45815182e+00, -1.28516540e-01, -7.14540064e-01,\n",
              "        -2.32113063e-01,  3.51573884e-01, -7.47285175e+00,\n",
              "         1.21313417e+00],\n",
              "       [-1.38755369e+00, -3.19472528e+00, -2.82719463e-01,\n",
              "         4.38774910e+01, -1.32771158e+00,  2.28094234e+01,\n",
              "         2.29193187e+00,  3.88743067e+00,  1.91648617e+01,\n",
              "         8.55238512e-02, -1.00003786e-01,  1.38982553e+01,\n",
              "         7.38786030e+00, -4.45868164e-01,  9.85829830e+00,\n",
              "        -2.56518221e+00],\n",
              "       [ 1.96641338e+00, -1.25705349e+00, -1.00664735e-01,\n",
              "        -5.92371607e+00,  6.19701672e+00, -3.25666642e+00,\n",
              "        -6.12419844e-01, -1.83608711e+00, -3.20142889e+00,\n",
              "        -9.17595804e-01, -9.44501013e-02, -1.66527176e+00,\n",
              "        -1.02702367e+00, -2.70320028e-01, -5.50899076e+00,\n",
              "         1.56630874e+00],\n",
              "       [ 7.49594986e-01, -2.13326812e+00,  2.91212022e-01,\n",
              "        -8.36106014e+00,  5.79364491e+00, -4.18773890e+00,\n",
              "        -2.94867009e-01, -4.95073271e+00, -3.66431785e+00,\n",
              "        -3.83631110e-01,  1.10656410e-01, -2.41903162e+00,\n",
              "        -1.35695946e+00, -3.15118998e-01, -5.22362375e+00,\n",
              "         9.43990231e-01],\n",
              "       [ 6.49545622e+00,  3.34239781e-01,  2.45809570e-01,\n",
              "        -8.49887180e+00, -6.14619875e+00, -4.10799360e+00,\n",
              "        -9.59925771e-01, -1.14290333e+00, -3.66554904e+00,\n",
              "        -2.04778910e+00, -1.87685728e-01, -2.43652964e+00,\n",
              "        -1.31391335e+00, -2.07986817e-01, -1.70598701e-01,\n",
              "        -6.48273134e+00],\n",
              "       [-8.23366833e+00,  1.72937286e+00,  2.94331819e-01,\n",
              "        -4.48921919e+00, -6.07051182e+00, -2.08139038e+00,\n",
              "        -3.17447281e+00, -2.54406035e-03, -1.96957624e+00,\n",
              "        -1.84355211e+00,  8.27760622e-02, -1.13856244e+00,\n",
              "        -6.47333682e-01, -1.14833266e-01, -6.17620289e-01,\n",
              "         6.00331068e+00]], dtype=float32), array([  0.63194954,  -0.746495  ,  -0.28349447, -27.152506  ,\n",
              "         0.16080663, -14.546691  ,  -4.127557  ,  -8.613117  ,\n",
              "       -12.478163  ,  -5.6200485 ,  -0.39033306,  -8.84877   ,\n",
              "        -4.508902  ,  -0.27646363, -12.515848  ,   1.5303473 ],\n",
              "      dtype=float32), array([[ 4.03167993e-01,  3.72543859e+00,  4.44425404e-01,\n",
              "         8.69023800e-01, -4.84854043e-01,  3.02957511e+00],\n",
              "       [-9.80849117e-02,  5.68757629e+00,  4.01443303e-01,\n",
              "         1.82750356e+00, -1.65680751e-01,  4.23638821e+00],\n",
              "       [ 1.60646170e-01, -1.72010824e-01, -4.15750027e-01,\n",
              "        -4.92064565e-01,  3.17889273e-01, -1.67369191e-02],\n",
              "       [-4.94890898e-01,  1.91902618e+01,  5.08938789e-01,\n",
              "         5.48317099e+00,  1.01023607e-01,  1.43108101e+01],\n",
              "       [ 4.88215312e-02,  4.72477865e+00,  5.19004285e-01,\n",
              "         1.64721894e+00, -3.30612004e-01,  2.98644114e+00],\n",
              "       [ 9.94800706e-04,  1.04092398e+01, -1.74690187e-01,\n",
              "         2.30523419e+00, -1.23921566e-01,  7.02822733e+00],\n",
              "       [ 2.02560395e-01,  1.98820686e+00, -6.19369507e-01,\n",
              "         1.80915326e-01, -4.47760880e-01,  1.28841138e+00],\n",
              "       [-1.01312660e-01,  4.18405867e+00, -3.86854976e-01,\n",
              "         1.37536335e+00, -2.69758135e-01,  3.19017768e+00],\n",
              "       [ 4.16321844e-01,  8.09466076e+00, -1.42036855e-01,\n",
              "         2.63010335e+00,  1.91892996e-01,  6.61862421e+00],\n",
              "       [-3.22671264e-01,  4.79665184e+00, -6.29556954e-01,\n",
              "         1.35113490e+00, -3.47614616e-01,  3.73181343e+00],\n",
              "       [-2.12347031e-01, -1.93790600e-01,  2.33426273e-01,\n",
              "        -4.74488407e-01, -2.82051396e-02, -4.64901924e-01],\n",
              "       [ 3.20539474e-01,  6.25493383e+00, -2.23193362e-01,\n",
              "         1.31923497e+00, -3.91715497e-01,  4.44063950e+00],\n",
              "       [-9.41663235e-02,  3.13706493e+00,  1.65830657e-01,\n",
              "         1.21158886e+00,  1.12811126e-01,  2.36872983e+00],\n",
              "       [ 3.51814747e-01, -3.85321677e-01,  4.85153049e-01,\n",
              "         5.04308045e-01,  2.88559794e-01, -4.44349915e-01],\n",
              "       [-4.80905294e-01,  5.62732553e+00, -8.31401944e-01,\n",
              "         1.18052101e+00, -2.28328809e-01,  3.65256929e+00],\n",
              "       [ 1.48080707e-01,  3.45797610e+00,  7.27638543e-01,\n",
              "         5.46128333e-01,  7.36383423e-02,  3.18997359e+00]], dtype=float32), array([ 0.08754469, 21.920204  ,  0.25253397,  6.009892  , -0.06167934,\n",
              "       16.159208  ], dtype=float32), array([[ 0.3633789 ],\n",
              "       [21.407612  ],\n",
              "       [ 0.8106544 ],\n",
              "       [ 5.8869777 ],\n",
              "       [-0.67019856],\n",
              "       [15.880208  ]], dtype=float32), array([1.7685485], dtype=float32)], non_trainable=[]), distributor=(), client_work=(), aggregator=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), finalizer=[149, array([[ 6.2110223e-02,  6.8373963e-02,  0.0000000e+00, -1.8586994e-03,\n",
              "         3.1819154e-02, -4.0451381e-03,  1.7806483e-02,  2.1256877e-02,\n",
              "        -5.6820996e-03,  9.9153044e-03, -1.8498870e-04, -9.9727360e-05,\n",
              "         2.2660273e-04, -4.8346716e-04,  2.5909042e-02,  3.8748924e-02],\n",
              "       [ 5.4236553e-03,  3.4122240e-02,  0.0000000e+00,  2.6349811e-02,\n",
              "         1.3948812e-02,  1.2422598e-02, -4.8002764e-03, -4.3491577e-03,\n",
              "         7.9861693e-03, -5.2577704e-03,  0.0000000e+00,  5.8926330e-03,\n",
              "         5.2453312e-03, -1.4434420e-04,  4.7474848e-03, -2.0103149e-02],\n",
              "       [-3.9959248e-02, -3.5850659e-02,  0.0000000e+00,  1.3902916e-01,\n",
              "         9.6146353e-03,  7.2131090e-02, -1.0195052e-02, -2.6191771e-02,\n",
              "         5.9337188e-02, -4.8879471e-02, -6.0410424e-05,  4.2370465e-02,\n",
              "         2.4100637e-02, -3.7842727e-04, -9.1139311e-03,  9.6044904e-03],\n",
              "       [ 4.3731343e-02,  4.2916358e-02,  0.0000000e+00, -4.9340492e-03,\n",
              "         1.1288788e-02, -2.3105112e-04, -6.0042548e-03, -3.7542745e-02,\n",
              "         3.9613858e-04, -4.9153911e-03,  0.0000000e+00, -1.3070566e-03,\n",
              "        -6.6982175e-04, -7.8475912e-04, -1.4097089e-02,  1.8921230e-02],\n",
              "       [-1.8297486e-02, -1.4460975e-01,  0.0000000e+00,  1.1856427e-01,\n",
              "        -2.7402727e-02,  6.2853597e-02, -2.4640476e-02, -5.5758279e-02,\n",
              "         5.5209685e-02, -6.4335547e-02,  0.0000000e+00,  3.8186789e-02,\n",
              "         1.9030213e-02,  0.0000000e+00, -7.2005562e-02, -7.5803958e-02],\n",
              "       [-1.3396881e-02,  1.2122533e-02,  0.0000000e+00, -2.7602771e-02,\n",
              "         3.0106032e-02, -1.2945148e-02, -1.1990992e-02, -4.8741780e-02,\n",
              "        -1.0078434e-02, -5.5467095e-02,  0.0000000e+00, -8.2050525e-03,\n",
              "        -4.5808377e-03,  0.0000000e+00, -4.7548085e-02, -2.7215993e-02],\n",
              "       [-4.4248037e-02,  2.8549973e-04,  0.0000000e+00, -5.9784479e-02,\n",
              "         3.7632313e-02, -3.0688390e-02, -7.9894690e-03, -2.3910191e-02,\n",
              "        -2.5685895e-02, -7.8039574e-03, -3.5382315e-04, -1.8727986e-02,\n",
              "        -1.0151169e-02,  0.0000000e+00, -3.0939046e-03,  1.8001549e-02],\n",
              "       [ 3.1364560e-02, -2.7122905e-02,  0.0000000e+00, -3.9828783e-03,\n",
              "        -4.4177350e-02,  2.1347492e-03,  2.3157455e-03, -8.3921039e-03,\n",
              "         2.4066209e-03, -2.1080254e-02,  0.0000000e+00, -3.8926061e-03,\n",
              "         3.4030550e-04,  5.6934357e-05,  2.0745499e-02, -3.7741669e-02],\n",
              "       [-1.9416559e-02, -2.6571723e-03,  0.0000000e+00, -4.0113926e-02,\n",
              "        -2.5175883e-02, -2.6885722e-02,  6.0329582e-03,  5.0124139e-02,\n",
              "        -2.6070705e-02,  5.0184377e-02,  5.2617204e-05, -1.2708478e-02,\n",
              "        -6.9173039e-03, -9.8242564e-04,  6.0350872e-03,  4.1242521e-02]],\n",
              "      dtype=float32), array([-0.04569699, -0.01737203,  0.        , -0.1314842 , -0.00161483,\n",
              "       -0.06838431, -0.01163178, -0.03091992, -0.05942849, -0.03416691,\n",
              "       -0.00030121, -0.04353405, -0.02130908, -0.00092549, -0.0238613 ,\n",
              "       -0.00571348], dtype=float32), array([[ 2.2053102e-03, -2.5688691e-02,  8.3097717e-04, -7.0891799e-03,\n",
              "         0.0000000e+00, -1.9003762e-02],\n",
              "       [ 2.9865450e-03, -1.2559875e-02, -1.1477292e-03, -3.5948497e-03,\n",
              "         2.3863239e-04, -9.9458639e-03],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         0.0000000e+00,  0.0000000e+00],\n",
              "       [-6.3132838e-04,  3.1185632e-03, -2.4841714e-03,  1.4302792e-03,\n",
              "        -6.5809552e-04,  3.3757805e-03],\n",
              "       [ 2.7098511e-03,  2.3607519e-02,  2.4121881e-03,  6.0735731e-03,\n",
              "         0.0000000e+00,  1.7482998e-02],\n",
              "       [-3.1912071e-04,  3.3496591e-04, -1.3079020e-03,  3.8331453e-04,\n",
              "        -3.4091802e-04,  7.9849921e-04],\n",
              "       [-1.8412255e-06, -6.2559280e-03, -2.4112384e-03, -1.6846411e-03,\n",
              "         0.0000000e+00, -4.6202159e-03],\n",
              "       [ 1.7622924e-04, -7.7965381e-03, -4.7086854e-03, -1.9611851e-03,\n",
              "        -1.1040271e-05, -5.6295609e-03],\n",
              "       [-2.5203600e-04,  3.4359051e-04, -1.1200785e-03,  3.3939787e-04,\n",
              "        -2.7978065e-04,  7.1036810e-04],\n",
              "       [-2.1159089e-04, -3.4784522e-02, -6.2693912e-03, -9.5939375e-03,\n",
              "        -1.0009855e-06, -2.6104311e-02],\n",
              "       [ 0.0000000e+00,  2.1411106e-06,  1.7322600e-08,  6.2696637e-07,\n",
              "         0.0000000e+00,  1.5418976e-06],\n",
              "       [-1.8983633e-04,  9.1246975e-04, -7.5561710e-04,  4.2931139e-04,\n",
              "        -2.1046773e-04,  1.0116252e-03],\n",
              "       [-1.0627033e-04,  4.9529376e-04, -4.4083144e-04,  2.3236718e-04,\n",
              "        -1.1413079e-04,  5.4851471e-04],\n",
              "       [ 1.5273690e-08,  1.7729402e-05,  1.1254102e-06,  4.9076975e-06,\n",
              "         0.0000000e+00,  1.3086573e-05],\n",
              "       [-3.7264079e-04, -3.1357646e-02, -7.0050391e-03, -8.2713878e-03,\n",
              "        -5.7034380e-05, -2.2849141e-02],\n",
              "       [ 1.7855921e-03,  2.2274882e-02,  2.1299676e-03,  6.0337069e-03,\n",
              "         3.7259443e-04,  1.5847821e-02]], dtype=float32), array([ 1.6897563e-03,  1.3081756e-01,  4.5646667e-03,  3.6069583e-02,\n",
              "       -1.0872419e-04,  9.6961163e-02], dtype=float32), array([[2.7988809e-03],\n",
              "       [2.8760914e-02],\n",
              "       [9.2440657e-03],\n",
              "       [8.3372882e-03],\n",
              "       [5.9705973e-05],\n",
              "       [2.0716239e-02]], dtype=float32), array([0.00959998], dtype=float32)])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "result.state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWHGNcN-VA98"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1qAVD1haVA9-"
      },
      "outputs": [],
      "source": [
        "# Create the test data for model evaluation\n",
        "X_test = pd.concat([f[1][0] for f in federated_insurance_data])\n",
        "y_test = pd.concat([f[1][1] for f in federated_insurance_data])\n",
        "\n",
        "test_sets = [tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.convert_to_tensor(np.expand_dims(el[1][0], axis=0)), \n",
        "    tf.convert_to_tensor(np.expand_dims(el[1][1], axis=0)))) \n",
        "    for el in federated_insurance_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "xHTiYD-nVA9_",
        "outputId": "cae86336-da7d-4465-8822-aab2e71251a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-fe93b6610415>:2: DeprecationWarning: `tff.learning.build_federated_evaluation` is deprecated, use `tff.learning.algorithms.build_fed_eval` instead.\n",
            "  evaluation = tff.learning.build_federated_evaluation(model_fn_5_mod)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-fe93b6610415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_federated_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_5_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(evaluation.type_signature.formatted_representation())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterative_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/common_libs/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__deprecated__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__deprecated__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/federated_evaluation.py\u001b[0m in \u001b[0;36mbuild_federated_evaluation\u001b[0;34m(model_fn, broadcast_process, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     return _build_federated_evaluation(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mbroadcast_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbroadcast_process\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/federated_evaluation.py\u001b[0m in \u001b[0;36m_build_federated_evaluation\u001b[0;34m(model_fn, broadcast_process, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m   local_eval = build_local_evaluation(\n\u001b[0m\u001b[1;32m    300\u001b[0m       \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mmodel_weights_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_weights_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/federated_evaluation.py\u001b[0m in \u001b[0;36mbuild_local_evaluation\u001b[0;34m(model_fn, model_weights_type, batch_type, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m     87\u001b[0m   )\n\u001b[1;32m     88\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mclient_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming_model_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;34m\"\"\"Returns local outputs after evaluting `model_weights` on `dataset`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tff_internal_types, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0;31m# Either we have a concrete parameter type, or this is no-arg function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m       \u001b[0mparameter_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parameter_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       wrapped_func = self._strategy(\n\u001b[0m\u001b[1;32m    528\u001b[0m           \u001b[0mfn_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m           \u001b[0mfn_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn_to_wrap, fn_name, parameter_type, unpack, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_arguments_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_to_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mComputationReturnedNoneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_to_wrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/federated_evaluation.py\u001b[0m in \u001b[0;36mtf__client_eval\u001b[0;34m(incoming_model_weights, dataset)\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval__1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdataset_reduce_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_reduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset_reduce_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_experimental_simulation_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_reduce_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_local_unfinalized_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/dataset_reduce.py\u001b[0m in \u001b[0;36mtf___dataset_reduce_fn\u001b[0;34m(reduce_fn, dataset, initial_state_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/federated_evaluation.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[0;34m(num_examples, batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mdo_return_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0mretval__1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py\u001b[0m in \u001b[0;36mtf__forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py\u001b[0m in \u001b[0;36mtf___forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py\u001b[0m in \u001b[0;36mtf__predict_on_batch\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    299\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/federated_evaluation.py\", line 99, in reduce_fn  *\n        model_output = model.forward_pass(batch, training=False)\n    File \"/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/dataset_reduce.py\", line 29, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py\", line 503, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py\", line 452, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/usr/local/lib/python3.9/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py\", line 440, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 9)\n"
          ]
        }
      ],
      "source": [
        "# Model evaluation\n",
        "evaluation = tff.learning.build_federated_evaluation(model_fn_5_mod)\n",
        "# print(evaluation.type_signature.formatted_representation())\n",
        "model_weights = iterative_process.get_model_weights(state)\n",
        "train_metrics = evaluation(model_weights, test_sets)\n",
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NKLXp9aiDqA"
      },
      "outputs": [],
      "source": [
        "r2scr = RSquare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob4g5OHuVA-A"
      },
      "outputs": [],
      "source": [
        "# Create model from training results and evaluate\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "model = create_keras_model(input_features = 9)\n",
        "model_weights.assign_weights_to(model)\n",
        "model.compile(\n",
        "    loss=tf.losses.mae,\n",
        "    optimizer=tf.optimizers.SGD(),\n",
        "    metrics=[\"mae\", 'mean_squared_error'#, r2_score\n",
        "             ]\n",
        ")\n",
        "\n",
        "# The evaluation results, for technical reasons the metrics_names is called afterwards. However, its order fits to the results\n",
        "print(model.evaluate(X_test, y_test))\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_9XHZx0XddY"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5W1ESJXe4z"
      },
      "source": [
        "- the original repo uses a totally different model as in centralized learning.\n",
        "  - linear\n",
        "  - no learning by zero initializer\n",
        "- new learner cuts mae ~66%\n",
        "- how to calculate RSquare?\n",
        "- error: no learning occurs (for clients). Empty?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}